{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4cce01a6",
   "metadata": {},
   "source": [
    "# Assignment 2: Named Entity Recognition (NER)\n",
    "\n",
    "This notebook implements and compares two traditional NER approaches on a benchmark dataset.\n",
    "\n",
    "## Goals\n",
    "- Choose a benchmark NER dataset (WikiAnn English)\n",
    "- Train a CRF model with handcrafted features\n",
    "- Train a BiLSTM model as a neural baseline\n",
    "- Tune hyperparameters on validation data\n",
    "- Evaluate with entity-level precision, recall, and F1\n",
    "- Analyze common error patterns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1e726f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (run once)\n",
    "!pip -q install datasets seqeval sklearn-crfsuite torch tqdm pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7a6e27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Thomas/Desktop/Nat_Lang_Engr_Meth_Tools/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x112eb6db0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from datasets import load_dataset, load_from_disk\n",
    "from seqeval.metrics import classification_report, f1_score, precision_score, recall_score\n",
    "\n",
    "import sklearn_crfsuite\n",
    "from sklearn_crfsuite import metrics as crf_metrics\n",
    "\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13bd3c1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating validation split: 100%|██████████| 10000/10000 [00:00<00:00, 658374.12 examples/s]\n",
      "Generating test split: 100%|██████████| 10000/10000 [00:00<00:00, 928580.22 examples/s]\n",
      "Generating train split: 100%|██████████| 20000/20000 [00:00<00:00, 850702.58 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: wikiann-en\n",
      "Labels: ['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC']\n",
      "Train size: 20000\n",
      "Validation size: 10000\n",
      "Test size: 10000\n"
     ]
    }
   ],
   "source": [
    "# Load WikiAnn English from local disk (fallback to Hugging Face if missing)\n",
    "local_path = Path(\"Assignment2_Name_Entity_Recognition/wikiann-en\")\n",
    "if local_path.exists():\n",
    "    raw = load_from_disk(str(local_path))\n",
    "    dataset_name = \"wikiann-en (local)\"\n",
    "else:\n",
    "    raw = load_dataset(\"wikiann\", \"en\")\n",
    "    dataset_name = \"wikiann-en\"\n",
    "\n",
    "label_list = raw[\"train\"].features[\"ner_tags\"].feature.names\n",
    "pos_list = raw[\"train\"].features[\"pos_tags\"].feature.names if \"pos_tags\" in raw[\"train\"].features else None\n",
    "\n",
    "print(\"Dataset:\", dataset_name)\n",
    "print(\"Labels:\", label_list)\n",
    "print(\"Train size:\", len(raw[\"train\"]))\n",
    "print(\"Validation size:\", len(raw[\"validation\"]))\n",
    "print(\"Test size:\", len(raw[\"test\"]))\n",
    "\n",
    "# Convert a record to (token, pos) sequence and label sequence\n",
    "def to_sent(record):\n",
    "    tokens = record[\"tokens\"]\n",
    "    if pos_list is None:\n",
    "        pos = [\"X\"] * len(tokens)\n",
    "    else:\n",
    "        pos = [pos_list[i] for i in record[\"pos_tags\"]]\n",
    "    labels = [label_list[i] for i in record[\"ner_tags\"]]\n",
    "    return list(zip(tokens, pos)), labels\n",
    "\n",
    "train_sents = [to_sent(r) for r in raw[\"train\"]]\n",
    "val_sents = [to_sent(r) for r in raw[\"validation\"]]\n",
    "test_sents = [to_sent(r) for r in raw[\"test\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ed92634",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CRF params {'c1': 0.01, 'c2': 0.01} val F1 0.7665\n",
      "CRF params {'c1': 0.1, 'c2': 0.01} val F1 0.7727\n",
      "CRF params {'c1': 0.1, 'c2': 0.1} val F1 0.7776\n",
      "CRF params {'c1': 0.5, 'c2': 0.1} val F1 0.7766\n",
      "CRF params {'c1': 0.5, 'c2': 0.5} val F1 0.7717\n",
      "Best CRF val F1: 0.7776\n",
      "CRF test metrics\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         LOC     0.7070    0.7275    0.7171      4657\n",
      "         ORG     0.6469    0.5633    0.6022      4745\n",
      "         PER     0.7987    0.8071    0.8028      4556\n",
      "\n",
      "   micro avg     0.7198    0.6977    0.7086     13958\n",
      "   macro avg     0.7175    0.6993    0.7074     13958\n",
      "weighted avg     0.7165    0.6977    0.7060     13958\n",
      "\n",
      "CRF test P/R/F1: 0.7198 0.6977 0.7086\n"
     ]
    }
   ],
   "source": [
    "# CRF feature extraction\n",
    "\n",
    "def word2features(sent, i):\n",
    "    word, pos = sent[i]\n",
    "    features = {\n",
    "        \"bias\": 1.0,\n",
    "        \"word.lower\": word.lower(),\n",
    "        \"word.isupper\": word.isupper(),\n",
    "        \"word.istitle\": word.istitle(),\n",
    "        \"word.isdigit\": word.isdigit(),\n",
    "        \"pos\": pos,\n",
    "        \"suffix3\": word[-3:],\n",
    "        \"suffix2\": word[-2:],\n",
    "        \"prefix1\": word[:1],\n",
    "        \"prefix2\": word[:2],\n",
    "    }\n",
    "\n",
    "    if i > 0:\n",
    "        prev_word, prev_pos = sent[i - 1]\n",
    "        features.update({\n",
    "            \"-1:word.lower\": prev_word.lower(),\n",
    "            \"-1:word.istitle\": prev_word.istitle(),\n",
    "            \"-1:word.isupper\": prev_word.isupper(),\n",
    "            \"-1:pos\": prev_pos,\n",
    "        })\n",
    "    else:\n",
    "        features[\"BOS\"] = True\n",
    "\n",
    "    if i < len(sent) - 1:\n",
    "        next_word, next_pos = sent[i + 1]\n",
    "        features.update({\n",
    "            \"+1:word.lower\": next_word.lower(),\n",
    "            \"+1:word.istitle\": next_word.istitle(),\n",
    "            \"+1:word.isupper\": next_word.isupper(),\n",
    "            \"+1:pos\": next_pos,\n",
    "        })\n",
    "    else:\n",
    "        features[\"EOS\"] = True\n",
    "\n",
    "    return features\n",
    "\n",
    "\n",
    "def sent2features(sent):\n",
    "    return [word2features(sent, i) for i in range(len(sent))]\n",
    "\n",
    "\n",
    "def sent2labels(labels):\n",
    "    return labels\n",
    "\n",
    "X_train = [sent2features(s) for s, _ in train_sents]\n",
    "y_train = [sent2labels(l) for _, l in train_sents]\n",
    "X_val = [sent2features(s) for s, _ in val_sents]\n",
    "y_val = [sent2labels(l) for _, l in val_sents]\n",
    "X_test = [sent2features(s) for s, _ in test_sents]\n",
    "y_test = [sent2labels(l) for _, l in test_sents]\n",
    "\n",
    "# Hyperparameter tuning for CRF\n",
    "labels_no_o = [l for l in label_list if l != \"O\"]\n",
    "\n",
    "crf_params = [\n",
    "    {\"c1\": 0.01, \"c2\": 0.01},\n",
    "    {\"c1\": 0.1, \"c2\": 0.01},\n",
    "    {\"c1\": 0.1, \"c2\": 0.1},\n",
    "    {\"c1\": 0.5, \"c2\": 0.1},\n",
    "    {\"c1\": 0.5, \"c2\": 0.5},\n",
    "]\n",
    "\n",
    "best_crf = None\n",
    "best_f1 = -1.0\n",
    "for p in crf_params:\n",
    "    crf = sklearn_crfsuite.CRF(\n",
    "        algorithm=\"lbfgs\",\n",
    "        c1=p[\"c1\"],\n",
    "        c2=p[\"c2\"],\n",
    "        max_iterations=100,\n",
    "        all_possible_transitions=True,\n",
    "    )\n",
    "    crf.fit(X_train, y_train)\n",
    "    val_pred = crf.predict(X_val)\n",
    "    f1 = crf_metrics.flat_f1_score(\n",
    "        y_val, val_pred, average=\"weighted\", labels=labels_no_o\n",
    "    )\n",
    "    print(\"CRF params\", p, \"val F1\", round(f1, 4))\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_crf = crf\n",
    "\n",
    "print(\"Best CRF val F1:\", round(best_f1, 4))\n",
    "\n",
    "# Evaluate CRF on test\n",
    "crf_test_pred = best_crf.predict(X_test)\n",
    "print(\"CRF test metrics\")\n",
    "print(classification_report(y_test, crf_test_pred, digits=4))\n",
    "crf_test_f1 = f1_score(y_test, crf_test_pred)\n",
    "crf_test_p = precision_score(y_test, crf_test_pred)\n",
    "crf_test_r = recall_score(y_test, crf_test_pred)\n",
    "print(\"CRF test P/R/F1:\", round(crf_test_p, 4), round(crf_test_r, 4), round(crf_test_f1, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af85ec77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training config: {'embed_dim': 100, 'hidden_dim': 128, 'dropout': 0.2, 'lr': 0.001, 'batch_size': 32, 'epochs': 3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 625/625 [00:12<00:00, 51.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val F1: 0.381\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 625/625 [00:11<00:00, 54.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val F1: 0.4502\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 625/625 [00:11<00:00, 54.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val F1: 0.4678\n",
      "Training config: {'embed_dim': 100, 'hidden_dim': 256, 'dropout': 0.3, 'lr': 0.0005, 'batch_size': 32, 'epochs': 3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 625/625 [00:17<00:00, 35.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val F1: 0.2794\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 625/625 [00:17<00:00, 35.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val F1: 0.3684\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 625/625 [00:17<00:00, 35.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val F1: 0.4162\n",
      "Best BiLSTM val F1: 0.4678\n",
      "Best config: {'embed_dim': 100, 'hidden_dim': 128, 'dropout': 0.2, 'lr': 0.001, 'batch_size': 32, 'epochs': 3}\n",
      "BiLSTM test metrics\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         LOC     0.3771    0.6618    0.4804      4657\n",
      "         ORG     0.3800    0.4145    0.3965      4745\n",
      "         PER     0.5924    0.4482    0.5103      4556\n",
      "\n",
      "   micro avg     0.4222    0.5080    0.4611     13958\n",
      "   macro avg     0.4498    0.5082    0.4624     13958\n",
      "weighted avg     0.4484    0.5080    0.4617     13958\n",
      "\n",
      "BiLSTM test P/R/F1: 0.4222 0.508 0.4611\n"
     ]
    }
   ],
   "source": [
    "# BiLSTM model\n",
    "\n",
    "# Build vocab from training data\n",
    "PAD_TOKEN = \"<PAD>\"\n",
    "UNK_TOKEN = \"<UNK>\"\n",
    "PAD_TAG = \"<PAD>\"\n",
    "\n",
    "word_counts = {}\n",
    "for sent, _ in train_sents:\n",
    "    for w, _ in sent:\n",
    "        word_counts[w] = word_counts.get(w, 0) + 1\n",
    "\n",
    "vocab = [PAD_TOKEN, UNK_TOKEN] + sorted(word_counts.keys())\n",
    "word2id = {w: i for i, w in enumerate(vocab)}\n",
    "id2word = {i: w for w, i in word2id.items()}\n",
    "\n",
    "label_list_bilstm = label_list + [PAD_TAG]\n",
    "label2id = {l: i for i, l in enumerate(label_list_bilstm)}\n",
    "id2label = {i: l for l, i in label2id.items()}\n",
    "\n",
    "pad_id = word2id[PAD_TOKEN]\n",
    "pad_tag_id = label2id[PAD_TAG]\n",
    "\n",
    "class NERDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, items, word2id, label2id):\n",
    "        self.items = items\n",
    "        self.word2id = word2id\n",
    "        self.label2id = label2id\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.items)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sent, labels = self.items[idx]\n",
    "        tokens = [w for w, _ in sent]\n",
    "        x = [self.word2id.get(w, self.word2id[UNK_TOKEN]) for w in tokens]\n",
    "        y = [self.label2id[l] for l in labels]\n",
    "        return {\"tokens\": tokens, \"input_ids\": x, \"tag_ids\": y}\n",
    "\n",
    "\n",
    "def collate_batch(batch):\n",
    "    max_len = max(len(b[\"input_ids\"]) for b in batch)\n",
    "    input_ids = []\n",
    "    tag_ids = []\n",
    "    lengths = []\n",
    "    tokens = []\n",
    "    for b in batch:\n",
    "        l = len(b[\"input_ids\"])\n",
    "        lengths.append(l)\n",
    "        tokens.append(b[\"tokens\"])\n",
    "        input_ids.append(b[\"input_ids\"] + [pad_id] * (max_len - l))\n",
    "        tag_ids.append(b[\"tag_ids\"] + [pad_tag_id] * (max_len - l))\n",
    "    return {\n",
    "        \"tokens\": tokens,\n",
    "        \"input_ids\": torch.tensor(input_ids, dtype=torch.long),\n",
    "        \"tag_ids\": torch.tensor(tag_ids, dtype=torch.long),\n",
    "        \"lengths\": torch.tensor(lengths, dtype=torch.long),\n",
    "    }\n",
    "\n",
    "\n",
    "def build_loaders(batch_size):\n",
    "    train_ds = NERDataset(train_sents, word2id, label2id)\n",
    "    val_ds = NERDataset(val_sents, word2id, label2id)\n",
    "    test_ds = NERDataset(test_sents, word2id, label2id)\n",
    "    return (\n",
    "        DataLoader(train_ds, batch_size=batch_size, shuffle=True, collate_fn=collate_batch),\n",
    "        DataLoader(val_ds, batch_size=batch_size, shuffle=False, collate_fn=collate_batch),\n",
    "        DataLoader(test_ds, batch_size=batch_size, shuffle=False, collate_fn=collate_batch),\n",
    "    )\n",
    "\n",
    "\n",
    "class BiLSTMTagger(nn.Module):\n",
    "    def __init__(self, vocab_size, tag_size, embed_dim, hidden_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_id)\n",
    "        self.lstm = nn.LSTM(\n",
    "            embed_dim, hidden_dim, batch_first=True, bidirectional=True\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.classifier = nn.Linear(hidden_dim * 2, tag_size)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        x = self.embedding(input_ids)\n",
    "        x, _ = self.lstm(x)\n",
    "        x = self.dropout(x)\n",
    "        return self.classifier(x)\n",
    "\n",
    "\n",
    "def eval_bilstm(model, loader, device):\n",
    "    model.eval()\n",
    "    all_true = []\n",
    "    all_pred = []\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            lengths = batch[\"lengths\"].to(device)\n",
    "            logits = model(input_ids)\n",
    "            preds = logits.argmax(-1).cpu().tolist()\n",
    "            for i, pred_seq in enumerate(preds):\n",
    "                length = lengths[i].item()\n",
    "                true_seq = batch[\"tag_ids\"][i][:length].tolist()\n",
    "                all_true.append([id2label[t] for t in true_seq])\n",
    "                all_pred.append([id2label[p] for p in pred_seq[:length]])\n",
    "    return all_true, all_pred\n",
    "\n",
    "\n",
    "def train_bilstm(config):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    train_loader, val_loader, _ = build_loaders(config[\"batch_size\"])\n",
    "    model = BiLSTMTagger(\n",
    "        vocab_size=len(vocab),\n",
    "        tag_size=len(label_list_bilstm),\n",
    "        embed_dim=config[\"embed_dim\"],\n",
    "        hidden_dim=config[\"hidden_dim\"],\n",
    "        dropout=config[\"dropout\"],\n",
    "    ).to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config[\"lr\"])\n",
    "    loss_fn = nn.CrossEntropyLoss(ignore_index=pad_tag_id)\n",
    "\n",
    "    for epoch in range(config[\"epochs\"]):\n",
    "        model.train()\n",
    "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n",
    "            optimizer.zero_grad()\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            tag_ids = batch[\"tag_ids\"].to(device)\n",
    "            logits = model(input_ids)\n",
    "            loss = loss_fn(logits.view(-1, logits.size(-1)), tag_ids.view(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        val_true, val_pred = eval_bilstm(model, val_loader, device)\n",
    "        val_f1 = f1_score(val_true, val_pred)\n",
    "        print(\"Val F1:\", round(val_f1, 4))\n",
    "\n",
    "    return model\n",
    "\n",
    "bilstm_configs = [\n",
    "    {\"embed_dim\": 100, \"hidden_dim\": 128, \"dropout\": 0.2, \"lr\": 1e-3, \"batch_size\": 32, \"epochs\": 3},\n",
    "    {\"embed_dim\": 100, \"hidden_dim\": 256, \"dropout\": 0.3, \"lr\": 5e-4, \"batch_size\": 32, \"epochs\": 3},\n",
    "]\n",
    "\n",
    "best_bilstm = None\n",
    "best_bilstm_f1 = -1.0\n",
    "best_config = None\n",
    "\n",
    "for cfg in bilstm_configs:\n",
    "    print(\"Training config:\", cfg)\n",
    "    model = train_bilstm(cfg)\n",
    "    _, val_loader, _ = build_loaders(cfg[\"batch_size\"])\n",
    "    val_true, val_pred = eval_bilstm(\n",
    "        model, val_loader, torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    )\n",
    "    val_f1 = f1_score(val_true, val_pred)\n",
    "    if val_f1 > best_bilstm_f1:\n",
    "        best_bilstm_f1 = val_f1\n",
    "        best_bilstm = model\n",
    "        best_config = cfg\n",
    "\n",
    "print(\"Best BiLSTM val F1:\", round(best_bilstm_f1, 4))\n",
    "print(\"Best config:\", best_config)\n",
    "\n",
    "# Evaluate BiLSTM on test\n",
    "_, _, test_loader = build_loaders(best_config[\"batch_size\"])\n",
    "test_true, test_pred = eval_bilstm(\n",
    "    best_bilstm, test_loader, torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    ")\n",
    "print(\"BiLSTM test metrics\")\n",
    "print(classification_report(test_true, test_pred, digits=4))\n",
    "\n",
    "bilstm_test_f1 = f1_score(test_true, test_pred)\n",
    "bilstm_test_p = precision_score(test_true, test_pred)\n",
    "bilstm_test_r = recall_score(test_true, test_pred)\n",
    "print(\"BiLSTM test P/R/F1:\", round(bilstm_test_p, 4), round(bilstm_test_r, 4), round(bilstm_test_f1, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "553088a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CRF</td>\n",
       "      <td>0.719840</td>\n",
       "      <td>0.697664</td>\n",
       "      <td>0.708579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BiLSTM</td>\n",
       "      <td>0.422184</td>\n",
       "      <td>0.508024</td>\n",
       "      <td>0.461143</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    model  precision    recall        f1\n",
       "0     CRF   0.719840  0.697664  0.708579\n",
       "1  BiLSTM   0.422184  0.508024  0.461143"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compare model metrics\n",
    "results_df = pd.DataFrame(\n",
    "    [\n",
    "        {\"model\": \"CRF\", \"precision\": crf_test_p, \"recall\": crf_test_r, \"f1\": crf_test_f1},\n",
    "        {\"model\": \"BiLSTM\", \"precision\": bilstm_test_p, \"recall\": bilstm_test_r, \"f1\": bilstm_test_f1},\n",
    "    ]\n",
    ")\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0bc70d78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top error types:\n",
      "[('ORG', 4315), ('LOC', 2382), ('PER', 2051), ('O', 1606)]\n",
      "Sample misclassified tokens:\n",
      "            Eli  true=  B-ORG  pred=  B-PER\n",
      "          Stone  true=  I-ORG  pred=  I-PER\n",
      "             in  true=  I-ORG  pred=      O\n",
      "            the  true=  I-ORG  pred=      O\n",
      "         United  true=  I-ORG  pred=  B-LOC\n",
      "         States  true=  I-ORG  pred=  I-LOC\n",
      "          Willy  true=  B-PER  pred=      O\n",
      "          Unger  true=  I-PER  pred=      O\n",
      "        Palermo  true=  B-ORG  pred=  B-PER\n",
      "      Cathedral  true=  I-ORG  pred=  I-PER\n",
      "           List  true=  B-PER  pred=  B-ORG\n",
      "             of  true=  I-PER  pred=  I-ORG\n",
      "            The  true=  I-PER  pred=  I-ORG\n",
      "           O.C.  true=  I-PER  pred=  I-ORG\n",
      "     characters  true=  I-PER  pred=  I-ORG\n",
      "           Jack  true=  B-ORG  pred=  B-PER\n",
      "            and  true=  I-ORG  pred=  I-PER\n",
      "          Diane  true=  I-ORG  pred=  I-PER\n",
      "        2008–09  true=  B-ORG  pred=  B-LOC\n",
      "       Beşiktaş  true=  I-ORG  pred=  I-LOC\n"
     ]
    }
   ],
   "source": [
    "# Error analysis on CRF predictions\n",
    "\n",
    "def collect_errors(tokens_list, y_true, y_pred, max_examples=20):\n",
    "    errors = []\n",
    "    type_counts = {}\n",
    "    for tokens, true_seq, pred_seq in zip(tokens_list, y_true, y_pred):\n",
    "        for tok, t, p in zip(tokens, true_seq, pred_seq):\n",
    "            if t != p:\n",
    "                errors.append((tok, t, p))\n",
    "                t_type = t.split(\"-\")[-1] if \"-\" in t else t\n",
    "                type_counts[t_type] = type_counts.get(t_type, 0) + 1\n",
    "    return errors[:max_examples], type_counts\n",
    "\n",
    "# Use CRF outputs for analysis\n",
    "crf_tokens_test = [[w for w, _ in sent] for sent, _ in test_sents]\n",
    "error_examples, error_counts = collect_errors(crf_tokens_test, y_test, crf_test_pred)\n",
    "\n",
    "print(\"Top error types:\")\n",
    "print(sorted(error_counts.items(), key=lambda x: x[1], reverse=True)[:10])\n",
    "\n",
    "print(\"Sample misclassified tokens:\")\n",
    "for tok, t, p in error_examples:\n",
    "    print(f\"{tok:>15}  true={t:>7}  pred={p:>7}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563e9c0d",
   "metadata": {},
   "source": [
    "## Notes for the report\n",
    "- Record the best validation and test metrics from both models.\n",
    "- Summarize common error types and give plausible reasons.\n",
    "- Mention limitations (no character-level features, small hyperparameter sweep).\n",
    "- Suggest future work (BiLSTM-CRF, pretrained embeddings, contextual encoders)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
