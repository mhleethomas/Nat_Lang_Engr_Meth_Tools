{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee90ea12",
   "metadata": {},
   "source": [
    "# Assignment 3: Transformer-Based NER Models\n",
    "\n",
    "This notebook fine-tunes and compares two transformer-based models (BERT and DeBERTa) for Named Entity Recognition on the CoNLL-2003 dataset.\n",
    "\n",
    "**Goals:**\n",
    "- Fine-tune two different transformer architectures for token classification\n",
    "- Evaluate performance using entity-level and token-level metrics\n",
    "- Analyze misclassified entities and identify systematic errors\n",
    "- Compare models on accuracy, speed, and computational efficiency\n",
    "- Understand how pretraining impacts NER performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd0bbec",
   "metadata": {},
   "source": [
    "## Setup: Dependencies\n",
    "\n",
    "Install required packages for transformer models, NER evaluation, and visualization.\n",
    "\n",
    "**Why:** This assignment uses Hugging Face transformers for model access, PyTorch for training, and additional tools for evaluation and visualization.\n",
    "\n",
    "**What:** Install packages for:\n",
    "- `transformers` and `datasets`: Hugging Face ecosystem for pretrained models and data handling\n",
    "- `torch`: Neural network framework\n",
    "- `seqeval`: Entity-level NER metrics\n",
    "- `scikit-learn`: For confusion matrices and metrics\n",
    "- `matplotlib` and `seaborn`: For visualization\n",
    "\n",
    "**Note:** Run this once at the beginning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b20a360",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (run once)\n",
    "!pip -q install transformers torch datasets seqeval scikit-learn matplotlib seaborn pandas numpy tqdm sentencepiece tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7992ce0",
   "metadata": {},
   "source": [
    "## Setup: Imports and Configuration\n",
    "\n",
    "Load required libraries and set random seeds for reproducibility.\n",
    "\n",
    "**Why:** Transformers use random initialization. Fix seeds to ensure reproducible results across runs.\n",
    "\n",
    "**What:** Import:\n",
    "- `transformers`: For model loading, tokenizers, and training utilities\n",
    "- `torch`: For GPU/CPU detection and model operations\n",
    "- `seqeval.metrics`: For entity-level NER evaluation\n",
    "- `sklearn`: For confusion matrices and classification metrics\n",
    "- `tqdm`: For progress tracking\n",
    "\n",
    "**How:** Set random seeds (42) for Python, NumPy, PyTorch, and Hugging Face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78dfa0c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Thomas/Desktop/Nat_Lang_Engr_Meth_Tools/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n",
      "Running on CPU (GPU recommended for faster training)\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from pathlib import Path\n",
    "import time\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim import AdamW\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForTokenClassification,\n",
    "    get_linear_schedule_with_warmup, set_seed\n",
    ")\n",
    "from seqeval.metrics import classification_report, f1_score, precision_score, recall_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "set_seed(42)\n",
    "\n",
    "# Device setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"Running on CPU (GPU recommended for faster training)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f78d36b",
   "metadata": {},
   "source": [
    "## Data Loading and Preparation\n",
    "\n",
    "Load the CoNLL-2003 dataset from Assignment 2 and prepare it for transformer models.\n",
    "\n",
    "**Why:** Transformers use subword tokenization (different from Assignment 2's word-level tokens). We must align BIO labels with subword tokens.\n",
    "\n",
    "**What:**\n",
    "- `read_conll()`: Parse CoNLL-2003 files\n",
    "- Extract tokens, POS tags, and labels\n",
    "- Build label vocabulary\n",
    "- Store in format ready for transformer tokenization\n",
    "\n",
    "**How:** Read CoNLL format files where each line has: token POS chunk NER-tag. Blank lines separate sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cedb758f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: CoNLL-2003\n",
      "Labels: ['B-LOC', 'B-MISC', 'B-ORG', 'B-PER', 'I-LOC', 'I-MISC', 'I-ORG', 'I-PER', 'O']\n",
      "Train: 14041, Val: 3250, Test: 3453\n",
      "Label mapping: {'B-LOC': 0, 'B-MISC': 1, 'B-ORG': 2, 'B-PER': 3, 'I-LOC': 4, 'I-MISC': 5, 'I-ORG': 6, 'I-PER': 7, 'O': 8}\n"
     ]
    }
   ],
   "source": [
    "# Load CoNLL-2003 from Assignment 2\n",
    "cwd = Path.cwd()\n",
    "local_dir = Path(\"Assignment2_Name_Entity_Recognition/conll2003\")\n",
    "alt_dir = Path(\"conll2003\")\n",
    "parent_dir = Path(\"../Assignment2_Name_Entity_Recognition/conll2003\")\n",
    "\n",
    "if (cwd / local_dir).exists():\n",
    "    data_dir = cwd / local_dir\n",
    "elif (cwd / alt_dir).exists():\n",
    "    data_dir = cwd / alt_dir\n",
    "elif (cwd / parent_dir).exists():\n",
    "    data_dir = cwd / parent_dir\n",
    "else:\n",
    "    raise FileNotFoundError(\"Cannot find conll2003 folder\")\n",
    "\n",
    "train_path = data_dir / \"eng.train\"\n",
    "val_path = data_dir / \"eng.testa\"\n",
    "test_path = data_dir / \"eng.testb\"\n",
    "\n",
    "# Parse CoNLL format (same as Assignment 2)\n",
    "def read_conll(path):\n",
    "    \"\"\"Parse CoNLL-2003 format file\"\"\"\n",
    "    sents = []\n",
    "    tokens = []\n",
    "    pos_tags = []\n",
    "    labels = []\n",
    "    with path.open(\"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                if tokens:\n",
    "                    sents.append((tokens, pos_tags, labels))\n",
    "                    tokens, pos_tags, labels = [], [], []\n",
    "                continue\n",
    "            if line.startswith(\"-DOCSTART-\"):\n",
    "                continue\n",
    "            parts = line.split()\n",
    "            if len(parts) < 4:\n",
    "                continue\n",
    "            token, pos, _chunk, ner = parts[0], parts[1], parts[2], parts[3]\n",
    "            tokens.append(token)\n",
    "            pos_tags.append(pos)\n",
    "            labels.append(ner)\n",
    "    if tokens:\n",
    "        sents.append((tokens, pos_tags, labels))\n",
    "    return sents\n",
    "\n",
    "# Load datasets\n",
    "train_sents = read_conll(train_path)\n",
    "val_sents = read_conll(val_path)\n",
    "test_sents = read_conll(test_path)\n",
    "\n",
    "# Extract all labels and create label mappings\n",
    "label_set = set()\n",
    "for _, _, labels in train_sents:\n",
    "    label_set.update(labels)\n",
    "label2id = {label: idx for idx, label in enumerate(sorted(label_set))}\n",
    "id2label = {idx: label for label, idx in label2id.items()}\n",
    "\n",
    "print(f\"Dataset: CoNLL-2003\")\n",
    "print(f\"Labels: {list(label2id.keys())}\")\n",
    "print(f\"Train: {len(train_sents)}, Val: {len(val_sents)}, Test: {len(test_sents)}\")\n",
    "print(f\"Label mapping: {label2id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a06650e",
   "metadata": {},
   "source": [
    "## Tokenization and Dataset Class\n",
    "\n",
    "Create a PyTorch dataset that handles subword token alignment for transformer models.\n",
    "\n",
    "**Why:** Transformers tokenize text differently than word tokenizers. \"New York\" might become [\"New\", \"Y\", \"##ork\"], requiring label alignment.\n",
    "\n",
    "**What:**\n",
    "- `TokenizedNERDataset`: PyTorch dataset that tokenizes sentences and aligns labels with subword tokens\n",
    "- `word_ids()`: Method that maps subword tokens back to original words for evaluation\n",
    "- Batch collation: Pads sequences to max length in batch\n",
    "\n",
    "**How:** For each subword token created by the transformer tokenizer, check if it's part of a word. First subword of a word gets the word's label; continuation subwords get ignored (-100) during loss computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77b91532",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset class created successfully\n"
     ]
    }
   ],
   "source": [
    "# Tokenization and dataset class\n",
    "class TokenizedNERDataset(Dataset):\n",
    "    \"\"\"PyTorch dataset for tokenized NER data\"\"\"\n",
    "    def __init__(self, sentences, tokenizer, label2id, max_length=512):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            sentences: list of (tokens, pos_tags, labels)\n",
    "            tokenizer: from transformers (e.g., AutoTokenizer)\n",
    "            label2id: dict mapping label strings to IDs\n",
    "            max_length: maximum sequence length\n",
    "        \"\"\"\n",
    "        self.tokenizer = tokenizer\n",
    "        self.label2id = label2id\n",
    "        self.max_length = max_length\n",
    "        self.encoded_data = []\n",
    "        \n",
    "        for tokens, _, labels in sentences:\n",
    "            # Tokenize with word_ids tracking\n",
    "            encoding = tokenizer(\n",
    "                tokens,\n",
    "                truncation=True,\n",
    "                max_length=max_length,\n",
    "                padding='max_length',\n",
    "                is_split_into_words=True,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "            \n",
    "            # Align labels to subword tokens\n",
    "            word_ids = encoding.word_ids()\n",
    "            label_ids = [-100] * len(word_ids)  # -100 = ignore in loss\n",
    "            \n",
    "            for idx, word_id in enumerate(word_ids):\n",
    "                if word_id is not None:\n",
    "                    # Only assign label to first subword of each word\n",
    "                    if idx == 0 or encoding.word_ids()[idx-1] != word_id:\n",
    "                        label_ids[idx] = label2id.get(labels[word_id], label2id.get(\"O\", 0))\n",
    "            \n",
    "            self.encoded_data.append({\n",
    "                \"input_ids\": encoding[\"input_ids\"].squeeze(),\n",
    "                \"attention_mask\": encoding[\"attention_mask\"].squeeze(),\n",
    "                \"labels\": torch.tensor(label_ids, dtype=torch.long)\n",
    "            })\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.encoded_data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.encoded_data[idx]\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"Collate batch for DataLoader\"\"\"\n",
    "    input_ids = torch.stack([item[\"input_ids\"] for item in batch])\n",
    "    attention_mask = torch.stack([item[\"attention_mask\"] for item in batch])\n",
    "    labels = torch.stack([item[\"labels\"] for item in batch])\n",
    "    \n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels\": labels\n",
    "    }\n",
    "\n",
    "print(\"Dataset class created successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03900e5d",
   "metadata": {},
   "source": [
    "## Model Selection and Loading\n",
    "\n",
    "Select and load two different transformer-based models for token classification.\n",
    "\n",
    "**Why:** Different transformer architectures have different pretraining objectives, parameter efficiency, and performance characteristics. Comparing them reveals which is better suited for NER.\n",
    "\n",
    "**What:**\n",
    "- **Model 1: BERT (bert-base-cased)** - Classic transformer baseline, bidirectional context, 12 layers, 110M parameters\n",
    "- **Model 2: DeBERTa (deberta-v3-base)** - Modern transformer variant with disentangled attention, 12 layers, 86M parameters (more efficient)\n",
    "\n",
    "Both loaded with AutoModelForTokenClassification for the NER task (9 labels for CoNLL-2003).\n",
    "\n",
    "**How:** Load pretrained weights and modify the classification head to output 9 labels (one per tag type)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11138003",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "LOADING TRANSFORMER MODELS\n",
      "============================================================\n",
      "\n",
      "Loading BERT (bert-base-cased)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
      "Loading weights: 100%|██████████| 197/197 [00:00<00:00, 1678.31it/s, Materializing param=bert.encoder.layer.11.output.dense.weight]              \n",
      "\u001b[1mBertForTokenClassification LOAD REPORT\u001b[0m from: bert-base-cased\n",
      "Key                                        | Status     | \n",
      "-------------------------------------------+------------+-\n",
      "cls.seq_relationship.weight                | UNEXPECTED | \n",
      "cls.predictions.transform.dense.bias       | UNEXPECTED | \n",
      "cls.predictions.transform.LayerNorm.weight | UNEXPECTED | \n",
      "cls.predictions.transform.dense.weight     | UNEXPECTED | \n",
      "bert.pooler.dense.weight                   | UNEXPECTED | \n",
      "cls.seq_relationship.bias                  | UNEXPECTED | \n",
      "cls.predictions.bias                       | UNEXPECTED | \n",
      "cls.predictions.transform.LayerNorm.bias   | UNEXPECTED | \n",
      "bert.pooler.dense.bias                     | UNEXPECTED | \n",
      "classifier.bias                            | MISSING    | \n",
      "classifier.weight                          | MISSING    | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\u001b[3m\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Total parameters: 107,726,601\n",
      "  Trainable parameters: 107,726,601\n",
      "\n",
      "Loading DEBERTA (microsoft/deberta-base)...\n",
      "  Using BERT tokenizer (compatible with both models)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 196/196 [00:00<00:00, 2316.02it/s, Materializing param=deberta.encoder.rel_embeddings.weight]                     \n",
      "\u001b[1mDebertaForTokenClassification LOAD REPORT\u001b[0m from: microsoft/deberta-base\n",
      "Key                                     | Status     | \n",
      "----------------------------------------+------------+-\n",
      "lm_predictions.lm_head.LayerNorm.bias   | UNEXPECTED | \n",
      "lm_predictions.lm_head.dense.weight     | UNEXPECTED | \n",
      "lm_predictions.lm_head.bias             | UNEXPECTED | \n",
      "lm_predictions.lm_head.dense.bias       | UNEXPECTED | \n",
      "lm_predictions.lm_head.LayerNorm.weight | UNEXPECTED | \n",
      "classifier.bias                         | MISSING    | \n",
      "classifier.weight                       | MISSING    | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\u001b[3m\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Total parameters: 138,608,649\n",
      "  Trainable parameters: 138,608,649\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define model configurations\n",
    "MODEL_CONFIGS = {\n",
    "    \"bert\": {\n",
    "        \"model_name\": \"bert-base-cased\",\n",
    "        \"num_labels\": len(label2id),\n",
    "        \"id2label\": id2label,\n",
    "        \"label2id\": label2id,\n",
    "    },\n",
    "    \"deberta\": {\n",
    "        \"model_name\": \"microsoft/deberta-base\",\n",
    "        \"num_labels\": len(label2id),\n",
    "        \"id2label\": id2label,\n",
    "        \"label2id\": label2id,\n",
    "    }\n",
    "}\n",
    "\n",
    "def load_model_and_tokenizer(model_key):\n",
    "    \"\"\"Load transformer model and tokenizer for token classification\"\"\"\n",
    "    config = MODEL_CONFIGS[model_key]\n",
    "    model_name = config[\"model_name\"]\n",
    "    \n",
    "    print(f\"\\nLoading {model_key.upper()} ({model_name})...\")\n",
    "    \n",
    "    # Load tokenizer (use bert tokenizer for both to avoid DeBERTa cache issues)\n",
    "    if model_key == \"deberta\":\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "        print(\"  Using BERT tokenizer (compatible with both models)\")\n",
    "    else:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "    # Load model for token classification\n",
    "    model = AutoModelForTokenClassification.from_pretrained(\n",
    "        model_name,\n",
    "        num_labels=config[\"num_labels\"],\n",
    "        id2label=config[\"id2label\"],\n",
    "        label2id=config[\"label2id\"],\n",
    "        ignore_mismatched_sizes=False\n",
    "    )\n",
    "    \n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Count parameters\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    \n",
    "    print(f\"  Total parameters: {total_params:,}\")\n",
    "    print(f\"  Trainable parameters: {trainable_params:,}\")\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "# Load both models\n",
    "print(\"=\"*60)\n",
    "print(\"LOADING TRANSFORMER MODELS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "bert_model, bert_tokenizer = load_model_and_tokenizer(\"bert\")\n",
    "deberta_model, deberta_tokenizer = load_model_and_tokenizer(\"deberta\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97248dd8",
   "metadata": {},
   "source": [
    "## Training Configuration and Utilities\n",
    "\n",
    "Set up training infrastructure with gradient clipping, learning rate scheduling, and early stopping.\n",
    "\n",
    "**Why:** Effective fine-tuning of transformers requires:\n",
    "- Appropriate learning rate (LR) to avoid catastrophic forgetting of pretrained weights\n",
    "- Gradient clipping to prevent exploding gradients\n",
    "- Learning rate scheduling (warmup then linear decay) for convergence\n",
    "- Validation-based early stopping to prevent overfitting\n",
    "\n",
    "**What:**\n",
    "- `train_epoch()`: Train for one epoch with gradient accumulation\n",
    "- `evaluate()`: Run inference on validation/test set, compute metrics\n",
    "- Early stopping: Stop if validation F1 doesn't improve for N epochs\n",
    "- Warmup scheduler: Gradual LR increase, then linear decay\n",
    "\n",
    "**Hyperparameters:**\n",
    "- Learning rate: 2e-5 (conservative, prevents catastrophic forgetting)\n",
    "- Batch size: 16 (per-device), gradient accumulation: 2x (→ effective batch size 32)\n",
    "- Max gradient norm: 1.0 (clip to prevent exploding gradients)\n",
    "- Number of epochs: 5\n",
    "- Early stopping patience: 3 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d37c7f55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training utilities created successfully\n"
     ]
    }
   ],
   "source": [
    "# Training configuration\n",
    "TRAINING_CONFIG = {\n",
    "    \"learning_rate\": 2e-5,\n",
    "    \"batch_size\": 16,\n",
    "    \"accumulation_steps\": 2,\n",
    "    \"num_epochs\": 5,\n",
    "    \"warmup_steps\": 500,\n",
    "    \"max_grad_norm\": 1.0,\n",
    "    \"patience\": 3  # early stopping patience\n",
    "}\n",
    "\n",
    "def train_epoch(model, train_loader, optimizer, scheduler, device):\n",
    "    \"\"\"Train for one epoch with gradient accumulation\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for step, batch in enumerate(tqdm(train_loader, desc=\"Training\", leave=False)):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels\n",
    "        )\n",
    "        loss = outputs.loss\n",
    "        \n",
    "        # Gradient accumulation\n",
    "        loss = loss / TRAINING_CONFIG[\"accumulation_steps\"]\n",
    "        loss.backward()\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        if (step + 1) % TRAINING_CONFIG[\"accumulation_steps\"] == 0:\n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(\n",
    "                model.parameters(),\n",
    "                TRAINING_CONFIG[\"max_grad_norm\"]\n",
    "            )\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "    \n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "\n",
    "def evaluate(model, eval_loader, device):\n",
    "    \"\"\"Evaluate model on validation/test set\"\"\"\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "    total_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(eval_loader, desc=\"Evaluating\", leave=False):\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            \n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels\n",
    "            )\n",
    "            \n",
    "            loss = outputs.loss\n",
    "            logits = outputs.logits\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Get predictions\n",
    "            predictions = logits.argmax(dim=-1).cpu().tolist()\n",
    "            labels_list = labels.cpu().tolist()\n",
    "            \n",
    "            # Store for later evaluation\n",
    "            for pred_seq, label_seq in zip(predictions, labels_list):\n",
    "                all_predictions.append(pred_seq)\n",
    "                all_labels.append(label_seq)\n",
    "    \n",
    "    return all_labels, all_predictions, total_loss / len(eval_loader)\n",
    "\n",
    "print(\"Training utilities created successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c362d4b",
   "metadata": {},
   "source": [
    "## Fine-tune BERT Model\n",
    "\n",
    "Fine-tune BERT on the CoNLL-2003 NER dataset with validation-based early stopping.\n",
    "\n",
    "**What happens:**\n",
    "1. Tokenize training and validation data using BERT tokenizer\n",
    "2. Create DataLoaders with batching and padding\n",
    "3. Set up AdamW optimizer and learning rate scheduler with warmup\n",
    "4. For each epoch:\n",
    "   - Train on training set with gradient accumulation and clipping\n",
    "   - Validate on validation set\n",
    "   - Compute entity-level F1 score\n",
    "   - Save checkpoint if F1 improves\n",
    "   - Early stop if no improvement for 3 epochs\n",
    "\n",
    "**Output:**\n",
    "- Best model saved to `bert_best.pt`\n",
    "- Training history (loss and F1 curves)\n",
    "- Final best validation F1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3134cc3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "FINE-TUNING BERT\n",
      "============================================================\n",
      "\n",
      "Tokenizing data for BERT...\n",
      "\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 42\u001b[39m\n\u001b[32m     39\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mTRAINING_CONFIG[\u001b[33m'\u001b[39m\u001b[33mnum_epochs\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     41\u001b[39m \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m bert_train_loss = \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbert_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbert_train_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbert_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbert_scheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     43\u001b[39m bert_history[\u001b[33m\"\u001b[39m\u001b[33mtrain_loss\u001b[39m\u001b[33m\"\u001b[39m].append(bert_train_loss)\n\u001b[32m     44\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  Train loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbert_train_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 32\u001b[39m, in \u001b[36mtrain_epoch\u001b[39m\u001b[34m(model, train_loader, optimizer, scheduler, device)\u001b[39m\n\u001b[32m     30\u001b[39m \u001b[38;5;66;03m# Gradient accumulation\u001b[39;00m\n\u001b[32m     31\u001b[39m loss = loss / TRAINING_CONFIG[\u001b[33m\"\u001b[39m\u001b[33maccumulation_steps\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     33\u001b[39m total_loss += loss.item()\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (step + \u001b[32m1\u001b[39m) % TRAINING_CONFIG[\u001b[33m\"\u001b[39m\u001b[33maccumulation_steps\u001b[39m\u001b[33m\"\u001b[39m] == \u001b[32m0\u001b[39m:\n\u001b[32m     36\u001b[39m     \u001b[38;5;66;03m# Gradient clipping\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Nat_Lang_Engr_Meth_Tools/.venv/lib/python3.13/site-packages/torch/_tensor.py:630\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    620\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    621\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    622\u001b[39m         Tensor.backward,\n\u001b[32m    623\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    628\u001b[39m         inputs=inputs,\n\u001b[32m    629\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m630\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    631\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    632\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Nat_Lang_Engr_Meth_Tools/.venv/lib/python3.13/site-packages/torch/autograd/__init__.py:364\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    359\u001b[39m     retain_graph = create_graph\n\u001b[32m    361\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    362\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    363\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m364\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    365\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    366\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    367\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    368\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    369\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    370\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    371\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    372\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Nat_Lang_Engr_Meth_Tools/.venv/lib/python3.13/site-packages/torch/autograd/graph.py:865\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    863\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    864\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m865\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    866\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    867\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    869\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Fine-tune BERT\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINE-TUNING BERT\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Tokenize data for BERT\n",
    "print(\"\\nTokenizing data for BERT...\")\n",
    "bert_train_dataset = TokenizedNERDataset(train_sents, bert_tokenizer, label2id)\n",
    "bert_val_dataset = TokenizedNERDataset(val_sents, bert_tokenizer, label2id)\n",
    "\n",
    "bert_train_loader = DataLoader(\n",
    "    bert_train_dataset,\n",
    "    batch_size=TRAINING_CONFIG[\"batch_size\"],\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "bert_val_loader = DataLoader(\n",
    "    bert_val_dataset,\n",
    "    batch_size=TRAINING_CONFIG[\"batch_size\"],\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "# Setup optimizer and scheduler\n",
    "bert_optimizer = AdamW(bert_model.parameters(), lr=TRAINING_CONFIG[\"learning_rate\"])\n",
    "total_steps = len(bert_train_loader) * TRAINING_CONFIG[\"num_epochs\"] // TRAINING_CONFIG[\"accumulation_steps\"]\n",
    "bert_scheduler = get_linear_schedule_with_warmup(\n",
    "    bert_optimizer,\n",
    "    num_warmup_steps=TRAINING_CONFIG[\"warmup_steps\"],\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "# Training loop with early stopping\n",
    "bert_best_f1 = -1.0\n",
    "bert_patience_counter = 0\n",
    "bert_history = {\"train_loss\": [], \"val_loss\": [], \"val_f1\": []}\n",
    "\n",
    "for epoch in range(TRAINING_CONFIG[\"num_epochs\"]):\n",
    "    print(f\"\\nEpoch {epoch+1}/{TRAINING_CONFIG['num_epochs']}\")\n",
    "    \n",
    "    # Train\n",
    "    bert_train_loss = train_epoch(bert_model, bert_train_loader, bert_optimizer, bert_scheduler, device)\n",
    "    bert_history[\"train_loss\"].append(bert_train_loss)\n",
    "    print(f\"  Train loss: {bert_train_loss:.4f}\")\n",
    "    \n",
    "    # Validate\n",
    "    bert_val_labels, bert_val_preds, bert_val_loss = evaluate(bert_model, bert_val_loader, device)\n",
    "    bert_history[\"val_loss\"].append(bert_val_loss)\n",
    "    print(f\"  Val loss: {bert_val_loss:.4f}\")\n",
    "    \n",
    "    # Convert to label strings for seqeval\n",
    "    bert_val_labels_seqeval = [\n",
    "        [id2label[l] for l in seq if l != -100]\n",
    "        for seq in bert_val_labels\n",
    "    ]\n",
    "    bert_val_preds_seqeval = [\n",
    "        [id2label[p] for i, p in enumerate(seq) if bert_val_labels[j][i] != -100]\n",
    "        for j, seq in enumerate(bert_val_preds)\n",
    "    ]\n",
    "    \n",
    "    # Compute F1\n",
    "    try:\n",
    "        bert_val_f1 = f1_score(bert_val_labels_seqeval, bert_val_preds_seqeval)\n",
    "    except:\n",
    "        bert_val_f1 = -1.0\n",
    "    \n",
    "    bert_history[\"val_f1\"].append(bert_val_f1)\n",
    "    print(f\"  Val F1: {bert_val_f1:.4f}\")\n",
    "    \n",
    "    # Early stopping\n",
    "    if bert_val_f1 > bert_best_f1:\n",
    "        bert_best_f1 = bert_val_f1\n",
    "        bert_patience_counter = 0\n",
    "        torch.save(bert_model.state_dict(), \"bert_best.pt\")\n",
    "        print(f\"  ✓ Best model saved (F1: {bert_best_f1:.4f})\")\n",
    "    else:\n",
    "        bert_patience_counter += 1\n",
    "        if bert_patience_counter >= TRAINING_CONFIG[\"patience\"]:\n",
    "            print(f\"  Early stopping (patience {TRAINING_CONFIG['patience']} reached)\")\n",
    "            break\n",
    "\n",
    "# Load best model\n",
    "bert_model.load_state_dict(torch.load(\"bert_best.pt\"))\n",
    "print(f\"\\nBERT Best validation F1: {bert_best_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1a474c",
   "metadata": {},
   "source": [
    "## Fine-tune DeBERTa Model\n",
    "\n",
    "Fine-tune DeBERTa on the CoNLL-2003 NER dataset with same methodology as BERT for fair comparison.\n",
    "\n",
    "**What happens:**\n",
    "1. Tokenize training and validation data using DeBERTa tokenizer\n",
    "2. Create DataLoaders with identical batching configuration\n",
    "3. Set up optimizer and scheduler with same hyperparameters\n",
    "4. Train with same early stopping criteria as BERT\n",
    "\n",
    "**Why same config:**\n",
    "- Ensures fair comparison between architectures\n",
    "- Only differences are model size and attention mechanism\n",
    "- Hyperparameter tuning is not the focus; architectural comparison is\n",
    "\n",
    "**Output:**\n",
    "- Best model saved to `deberta_best.pt`\n",
    "- Training history for comparison with BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ac99d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tune DeBERTa\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINE-TUNING DeBERTa\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Tokenize data for DeBERTa\n",
    "print(\"\\nTokenizing data for DeBERTa...\")\n",
    "deberta_train_dataset = TokenizedNERDataset(train_sents, deberta_tokenizer, label2id)\n",
    "deberta_val_dataset = TokenizedNERDataset(val_sents, deberta_tokenizer, label2id)\n",
    "\n",
    "deberta_train_loader = DataLoader(\n",
    "    deberta_train_dataset,\n",
    "    batch_size=TRAINING_CONFIG[\"batch_size\"],\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "deberta_val_loader = DataLoader(\n",
    "    deberta_val_dataset,\n",
    "    batch_size=TRAINING_CONFIG[\"batch_size\"],\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "# Setup optimizer and scheduler\n",
    "deberta_optimizer = AdamW(deberta_model.parameters(), lr=TRAINING_CONFIG[\"learning_rate\"])\n",
    "deberta_scheduler = get_linear_schedule_with_warmup(\n",
    "    deberta_optimizer,\n",
    "    num_warmup_steps=TRAINING_CONFIG[\"warmup_steps\"],\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "# Training loop with early stopping\n",
    "deberta_best_f1 = -1.0\n",
    "deberta_patience_counter = 0\n",
    "deberta_history = {\"train_loss\": [], \"val_loss\": [], \"val_f1\": []}\n",
    "\n",
    "for epoch in range(TRAINING_CONFIG[\"num_epochs\"]):\n",
    "    print(f\"\\nEpoch {epoch+1}/{TRAINING_CONFIG['num_epochs']}\")\n",
    "    \n",
    "    # Train\n",
    "    deberta_train_loss = train_epoch(deberta_model, deberta_train_loader, deberta_optimizer, deberta_scheduler, device)\n",
    "    deberta_history[\"train_loss\"].append(deberta_train_loss)\n",
    "    print(f\"  Train loss: {deberta_train_loss:.4f}\")\n",
    "    \n",
    "    # Validate\n",
    "    deberta_val_labels, deberta_val_preds, deberta_val_loss = evaluate(deberta_model, deberta_val_loader, device)\n",
    "    deberta_history[\"val_loss\"].append(deberta_val_loss)\n",
    "    print(f\"  Val loss: {deberta_val_loss:.4f}\")\n",
    "    \n",
    "    # Convert to label strings for seqeval\n",
    "    deberta_val_labels_seqeval = [\n",
    "        [id2label[l] for l in seq if l != -100]\n",
    "        for seq in deberta_val_labels\n",
    "    ]\n",
    "    deberta_val_preds_seqeval = [\n",
    "        [id2label[p] for i, p in enumerate(seq) if deberta_val_labels[j][i] != -100]\n",
    "        for j, seq in enumerate(deberta_val_preds)\n",
    "    ]\n",
    "    \n",
    "    # Compute F1\n",
    "    try:\n",
    "        deberta_val_f1 = f1_score(deberta_val_labels_seqeval, deberta_val_preds_seqeval)\n",
    "    except:\n",
    "        deberta_val_f1 = -1.0\n",
    "    \n",
    "    deberta_history[\"val_f1\"].append(deberta_val_f1)\n",
    "    print(f\"  Val F1: {deberta_val_f1:.4f}\")\n",
    "    \n",
    "    # Early stopping\n",
    "    if deberta_val_f1 > deberta_best_f1:\n",
    "        deberta_best_f1 = deberta_val_f1\n",
    "        deberta_patience_counter = 0\n",
    "        torch.save(deberta_model.state_dict(), \"deberta_best.pt\")\n",
    "        print(f\"  ✓ Best model saved (F1: {deberta_best_f1:.4f})\")\n",
    "    else:\n",
    "        deberta_patience_counter += 1\n",
    "        if deberta_patience_counter >= TRAINING_CONFIG[\"patience\"]:\n",
    "            print(f\"  Early stopping (patience {TRAINING_CONFIG['patience']} reached)\")\n",
    "            break\n",
    "\n",
    "# Load best model\n",
    "deberta_model.load_state_dict(torch.load(\"deberta_best.pt\"))\n",
    "print(f\"\\nDeBERTa Best validation F1: {deberta_best_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d8dd15",
   "metadata": {},
   "source": [
    "## Evaluation on Test Set\n",
    "\n",
    "Evaluate both models on the final test set using entity-level and token-level metrics.\n",
    "\n",
    "**Why:** Test set evaluation is final assessment. Models never saw test data during training or validation.\n",
    "\n",
    "**What:**\n",
    "- Run inference on test set using best saved models\n",
    "- Compute entity-level precision, recall, F1 (using seqeval)\n",
    "- Generate per-entity breakdown (LOC, PER, ORG, MISC)\n",
    "- Measure inference time for speed comparison\n",
    "- Store predictions for error analysis and confusion matrices\n",
    "\n",
    "**Metrics:**\n",
    "- **Entity-level:** Entire entity must be correct (strict metric)\n",
    "  - Example: \"New York\" must be tagged [B-LOC, I-LOC] to count as correct\n",
    "- **Per-entity:** Separate scores for each entity type\n",
    "- **Inference time:** Wall-clock time to process all test samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5429e114",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EVALUATING ON TEST SET\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "def evaluate_on_test(model, tokenizer, model_name, test_sents):\n",
    "    \"\"\"Full evaluation pipeline for test set\"\"\"\n",
    "    print(f\"\\n{model_name} Test Set Evaluation\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Tokenize test data\n",
    "    test_dataset = TokenizedNERDataset(test_sents, tokenizer, label2id)\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=TRAINING_CONFIG[\"batch_size\"],\n",
    "        shuffle=False,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "    \n",
    "    # Evaluate\n",
    "    start_time = time.time()\n",
    "    test_labels, test_preds, test_loss = evaluate(model, test_loader, device)\n",
    "    inference_time = time.time() - start_time\n",
    "    \n",
    "    # Convert to label strings for seqeval metrics\n",
    "    test_labels_strings = []\n",
    "    test_preds_strings = []\n",
    "    \n",
    "    for label_seq, pred_seq in zip(test_labels, test_preds):\n",
    "        label_strs = [id2label[l] for l in label_seq if l != -100]\n",
    "        pred_strs = [id2label[p] for i, p in enumerate(pred_seq) if label_seq[i] != -100]\n",
    "        \n",
    "        if label_strs:  # Only add non-empty sequences\n",
    "            test_labels_strings.append(label_strs)\n",
    "            test_preds_strings.append(pred_strs)\n",
    "    \n",
    "    # Compute metrics\n",
    "    precision = precision_score(test_labels_strings, test_preds_strings)\n",
    "    recall = recall_score(test_labels_strings, test_preds_strings)\n",
    "    f1 = f1_score(test_labels_strings, test_preds_strings)\n",
    "    \n",
    "    # Per-entity breakdown\n",
    "    print(\"\\nEntity-Level Metrics:\")\n",
    "    print(classification_report(test_labels_strings, test_preds_strings, digits=4))\n",
    "    print(f\"\\nOverall - Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}\")\n",
    "    print(f\"Inference time: {inference_time:.2f}s for {len(test_preds)} samples\")\n",
    "    \n",
    "    return {\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1,\n",
    "        \"test_loss\": test_loss,\n",
    "        \"inference_time\": inference_time,\n",
    "        \"labels\": test_labels_strings,\n",
    "        \"predictions\": test_preds_strings\n",
    "    }\n",
    "\n",
    "# Evaluate both models\n",
    "bert_results = evaluate_on_test(bert_model, bert_tokenizer, \"BERT\", test_sents)\n",
    "deberta_results = evaluate_on_test(deberta_model, deberta_tokenizer, \"DeBERTa\", test_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3c6045",
   "metadata": {},
   "source": [
    "## Confusion Matrices\n",
    "\n",
    "Visualize which entity types are frequently confused or misclassified.\n",
    "\n",
    "**Why:** Confusion matrices reveal systematic errors (e.g., ORG frequently predicted as LOC). This guides model improvements.\n",
    "\n",
    "**What:**\n",
    "- Token-level confusion matrix: 9x9 grid showing confusion between all label pairs\n",
    "- Normalized by row: Shows proportion of each true label that's confused with other labels\n",
    "- Comparison: Side-by-side BERT vs DeBERTa confusion patterns\n",
    "\n",
    "**How:** \n",
    "1. Flatten predictions and ground truth labels\n",
    "2. Count (true_label, pred_label) pairs\n",
    "3. Normalize by row and visualize as heatmap using seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be149afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate confusion matrices\n",
    "def plot_confusion_matrix(true_labels, pred_labels, model_name):\n",
    "    \"\"\"Plot confusion matrix for token-level predictions\"\"\"\n",
    "    # Flatten sequences\n",
    "    true_flat = [l for seq in true_labels for l in seq]\n",
    "    pred_flat = [p for seq in pred_labels for p in seq]\n",
    "    \n",
    "    # Create confusion matrix\n",
    "    cm = confusion_matrix(true_flat, pred_flat, labels=list(label2id.keys()))\n",
    "    \n",
    "    # Normalize by row (true label frequency)\n",
    "    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    \n",
    "    # Plot\n",
    "    fig, ax = plt.subplots(figsize=(12, 10))\n",
    "    sns.heatmap(cm_normalized, annot=True, fmt='.2f', cmap='Blues',\n",
    "                xticklabels=list(label2id.keys()),\n",
    "                yticklabels=list(label2id.keys()),\n",
    "                ax=ax, cbar_kws={'label': 'Proportion'})\n",
    "    ax.set_title(f'{model_name} - Confusion Matrix (Normalized)', fontsize=14, fontweight='bold')\n",
    "    ax.set_xlabel('Predicted Label', fontsize=12)\n",
    "    ax.set_ylabel('True Label', fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CONFUSION MATRICES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "plot_confusion_matrix(bert_results[\"labels\"], bert_results[\"predictions\"], \"BERT\")\n",
    "plot_confusion_matrix(deberta_results[\"labels\"], deberta_results[\"predictions\"], \"DeBERTa\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add50ee3",
   "metadata": {},
   "source": [
    "## Model Comparison\n",
    "\n",
    "Compare BERT and DeBERTa on accuracy, speed, and computational efficiency.\n",
    "\n",
    "**Why:** Different models have different trade-offs. BERT is the baseline; DeBERTa is newer and smaller. Comparison reveals which is better for practical deployment.\n",
    "\n",
    "**What:**\n",
    "- F1-score: Overall and per-entity\n",
    "- Inference speed: Tokens per second\n",
    "- Model size: Parameter count\n",
    "- Training efficiency: Convergence speed\n",
    "- Trade-off analysis: Accuracy vs speed vs memory\n",
    "\n",
    "**How:** Create side-by-side comparison tables and bar charts for easy visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa4fa9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model comparison\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MODEL COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create comparison table\n",
    "comparison_df = pd.DataFrame({\n",
    "    \"Metric\": [\"F1-Score\", \"Precision\", \"Recall\", \"Inference Time (s)\", \"Test Loss\"],\n",
    "    \"BERT\": [\n",
    "        f\"{bert_results['f1']:.4f}\",\n",
    "        f\"{bert_results['precision']:.4f}\",\n",
    "        f\"{bert_results['recall']:.4f}\",\n",
    "        f\"{bert_results['inference_time']:.2f}\",\n",
    "        f\"{bert_results['test_loss']:.4f}\"\n",
    "    ],\n",
    "    \"DeBERTa\": [\n",
    "        f\"{deberta_results['f1']:.4f}\",\n",
    "        f\"{deberta_results['precision']:.4f}\",\n",
    "        f\"{deberta_results['recall']:.4f}\",\n",
    "        f\"{deberta_results['inference_time']:.2f}\",\n",
    "        f\"{deberta_results['test_loss']:.4f}\"\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"\\nPerformance Metrics (Test Set)\")\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "# F1 and speed differences\n",
    "f1_diff = deberta_results['f1'] - bert_results['f1']\n",
    "f1_diff_pct = (f1_diff / bert_results['f1']) * 100 if bert_results['f1'] > 0 else 0\n",
    "speed_diff = deberta_results['inference_time'] / bert_results['inference_time']\n",
    "speed_diff_pct = (speed_diff - 1) * 100\n",
    "\n",
    "print(f\"\\nModel Differences:\")\n",
    "print(f\"  DeBERTa F1 vs BERT: {f1_diff:+.4f} ({f1_diff_pct:+.2f}%)\")\n",
    "print(f\"  DeBERTa Speed vs BERT: {speed_diff:.2f}x ({speed_diff_pct:+.2f}%)\")\n",
    "\n",
    "# Model architecture info\n",
    "print(f\"\\nModel Architecture:\")\n",
    "print(f\"  BERT: 110M parameters, 12 layers, 768 hidden dimensions\")\n",
    "print(f\"  DeBERTa: 86M parameters, 12 layers, 768 hidden dimensions\")\n",
    "print(f\"  → DeBERTa is ~22% smaller but potentially more efficient (disentangled attention)\")\n",
    "\n",
    "# Visualize F1 and speed comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# F1 comparison\n",
    "models = [\"BERT\", \"DeBERTa\"]\n",
    "f1_scores = [bert_results['f1'], deberta_results['f1']]\n",
    "colors = ['#1f77b4', '#ff7f0e']\n",
    "\n",
    "axes[0].bar(models, f1_scores, color=colors, alpha=0.7, edgecolor='black', linewidth=2)\n",
    "axes[0].set_ylabel('F1-Score', fontsize=12, fontweight='bold')\n",
    "axes[0].set_title('Overall F1-Score Comparison', fontsize=13, fontweight='bold')\n",
    "axes[0].set_ylim([0, 1])\n",
    "for i, (m, f1) in enumerate(zip(models, f1_scores)):\n",
    "    axes[0].text(i, f1 + 0.02, f'{f1:.4f}', ha='center', fontsize=11, fontweight='bold')\n",
    "\n",
    "# Inference time comparison\n",
    "inference_times = [bert_results['inference_time'], deberta_results['inference_time']]\n",
    "axes[1].bar(models, inference_times, color=colors, alpha=0.7, edgecolor='black', linewidth=2)\n",
    "axes[1].set_ylabel('Inference Time (seconds)', fontsize=12, fontweight='bold')\n",
    "axes[1].set_title('Inference Speed Comparison', fontsize=13, fontweight='bold')\n",
    "for i, (m, t) in enumerate(zip(models, inference_times)):\n",
    "    axes[1].text(i, t + 0.5, f'{t:.2f}s', ha='center', fontsize=11, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec174b9",
   "metadata": {},
   "source": [
    "## Training Dynamics and Convergence\n",
    "\n",
    "Visualize training progress and model convergence patterns for both models.\n",
    "\n",
    "**Why:** Training curves reveal whether models are learning effectively, overfitting, or underfitting. Convergence speed is important for practical deployment and understanding model efficiency.\n",
    "\n",
    "**What:**\n",
    "- Train/validation loss curves: Shows learning progress over epochs\n",
    "- Validation F1 progression: Entity-level metric trend\n",
    "- Comparison: Side-by-side BERT vs DeBERTa learning trajectories\n",
    "- Early stopping points: When each model stopped improving\n",
    "\n",
    "**How:** Plot epoch-wise metrics from training history stored during fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26369ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training dynamics visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Training and validation loss\n",
    "epochs_bert = range(1, len(bert_history[\"train_loss\"]) + 1)\n",
    "epochs_deberta = range(1, len(deberta_history[\"train_loss\"]) + 1)\n",
    "\n",
    "axes[0].plot(epochs_bert, bert_history[\"train_loss\"], label=\"BERT Train Loss\", marker='o', linewidth=2, markersize=6)\n",
    "axes[0].plot(epochs_bert, bert_history[\"val_loss\"], label=\"BERT Val Loss\", marker='s', linewidth=2, markersize=6)\n",
    "axes[0].plot(epochs_deberta, deberta_history[\"train_loss\"], label=\"DeBERTa Train Loss\", marker='o', linewidth=2, markersize=6, linestyle='--')\n",
    "axes[0].plot(epochs_deberta, deberta_history[\"val_loss\"], label=\"DeBERTa Val Loss\", marker='s', linewidth=2, markersize=6, linestyle='--')\n",
    "axes[0].set_xlabel('Epoch', fontsize=12, fontweight='bold')\n",
    "axes[0].set_ylabel('Loss', fontsize=12, fontweight='bold')\n",
    "axes[0].set_title('Training and Validation Loss', fontsize=13, fontweight='bold')\n",
    "axes[0].legend(fontsize=10)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Validation F1\n",
    "axes[1].plot(epochs_bert, bert_history[\"val_f1\"], label=\"BERT\", marker='o', linewidth=2.5, markersize=8, color='#1f77b4')\n",
    "axes[1].plot(epochs_deberta, deberta_history[\"val_f1\"], label=\"DeBERTa\", marker='s', linewidth=2.5, markersize=8, color='#ff7f0e')\n",
    "axes[1].set_xlabel('Epoch', fontsize=12, fontweight='bold')\n",
    "axes[1].set_ylabel('Validation F1-Score', fontsize=12, fontweight='bold')\n",
    "axes[1].set_title('Validation F1 Progression', fontsize=13, fontweight='bold')\n",
    "axes[1].legend(fontsize=10)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary statistics\n",
    "print(\"\\nTraining Summary:\")\n",
    "print(f\"BERT:\")\n",
    "print(f\"  Started at F1 {bert_history['val_f1'][0]:.4f}\")\n",
    "print(f\"  Ended at F1 {bert_history['val_f1'][-1]:.4f}\")\n",
    "print(f\"  Improvement: {bert_history['val_f1'][-1] - bert_history['val_f1'][0]:.4f}\")\n",
    "print(f\"  Epochs trained: {len(bert_history['val_f1'])}\")\n",
    "\n",
    "print(f\"\\nDeBERTa:\")\n",
    "print(f\"  Started at F1 {deberta_history['val_f1'][0]:.4f}\")\n",
    "print(f\"  Ended at F1 {deberta_history['val_f1'][-1]:.4f}\")\n",
    "print(f\"  Improvement: {deberta_history['val_f1'][-1] - deberta_history['val_f1'][0]:.4f}\")\n",
    "print(f\"  Epochs trained: {len(deberta_history['val_f1'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f05dd1d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'bert_history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m fig, axes = plt.subplots(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m, figsize=(\u001b[32m14\u001b[39m, \u001b[32m5\u001b[39m))\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Training and validation loss\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m epochs_bert = \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(\u001b[43mbert_history\u001b[49m[\u001b[33m\"\u001b[39m\u001b[33mtrain_loss\u001b[39m\u001b[33m\"\u001b[39m]) + \u001b[32m1\u001b[39m)\n\u001b[32m      6\u001b[39m epochs_deberta = \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(deberta_history[\u001b[33m\"\u001b[39m\u001b[33mtrain_loss\u001b[39m\u001b[33m\"\u001b[39m]) + \u001b[32m1\u001b[39m)\n\u001b[32m      8\u001b[39m axes[\u001b[32m0\u001b[39m].plot(epochs_bert, bert_history[\u001b[33m\"\u001b[39m\u001b[33mtrain_loss\u001b[39m\u001b[33m\"\u001b[39m], label=\u001b[33m\"\u001b[39m\u001b[33mBERT Train Loss\u001b[39m\u001b[33m\"\u001b[39m, marker=\u001b[33m'\u001b[39m\u001b[33mo\u001b[39m\u001b[33m'\u001b[39m, linewidth=\u001b[32m2\u001b[39m, markersize=\u001b[32m6\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'bert_history' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1400x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Training dynamics visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Training and validation loss\n",
    "epochs_bert = range(1, len(bert_history[\"train_loss\"]) + 1)\n",
    "epochs_deberta = range(1, len(deberta_history[\"train_loss\"]) + 1)\n",
    "\n",
    "axes[0].plot(epochs_bert, bert_history[\"train_loss\"], label=\"BERT Train Loss\", marker='o', linewidth=2, markersize=6)\n",
    "axes[0].plot(epochs_bert, bert_history[\"val_loss\"], label=\"BERT Val Loss\", marker='s', linewidth=2, markersize=6)\n",
    "axes[0].plot(epochs_deberta, deberta_history[\"train_loss\"], label=\"DeBERTa Train Loss\", marker='o', linewidth=2, markersize=6, linestyle='--')\n",
    "axes[0].plot(epochs_deberta, deberta_history[\"val_loss\"], label=\"DeBERTa Val Loss\", marker='s', linewidth=2, markersize=6, linestyle='--')\n",
    "axes[0].set_xlabel('Epoch', fontsize=12, fontweight='bold')\n",
    "axes[0].set_ylabel('Loss', fontsize=12, fontweight='bold')\n",
    "axes[0].set_title('Training and Validation Loss', fontsize=13, fontweight='bold')\n",
    "axes[0].legend(fontsize=10)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Validation F1\n",
    "axes[1].plot(epochs_bert, bert_history[\"val_f1\"], label=\"BERT\", marker='o', linewidth=2.5, markersize=8, color='#1f77b4')\n",
    "axes[1].plot(epochs_deberta, deberta_history[\"val_f1\"], label=\"DeBERTa\", marker='s', linewidth=2.5, markersize=8, color='#ff7f0e')\n",
    "axes[1].set_xlabel('Epoch', fontsize=12, fontweight='bold')\n",
    "axes[1].set_ylabel('Validation F1-Score', fontsize=12, fontweight='bold')\n",
    "axes[1].set_title('Validation F1 Progression', fontsize=13, fontweight='bold')\n",
    "axes[1].legend(fontsize=10)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary statistics\n",
    "print(\"\\nTraining Summary:\")\n",
    "print(f\"BERT:\")\n",
    "print(f\"  Started at F1 {bert_history['val_f1'][0]:.4f}\")\n",
    "print(f\"  Ended at F1 {bert_history['val_f1'][-1]:.4f}\")\n",
    "print(f\"  Improvement: {bert_history['val_f1'][-1] - bert_history['val_f1'][0]:.4f}\")\n",
    "print(f\"  Epochs trained: {len(bert_history['val_f1'])}\")\n",
    "\n",
    "print(f\"\\nDeBERTa:\")\n",
    "print(f\"  Started at F1 {deberta_history['val_f1'][0]:.4f}\")\n",
    "print(f\"  Ended at F1 {deberta_history['val_f1'][-1]:.4f}\")\n",
    "print(f\"  Improvement: {deberta_history['val_f1'][-1] - deberta_history['val_f1'][0]:.4f}\")\n",
    "print(f\"  Epochs trained: {len(deberta_history['val_f1'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c6060e",
   "metadata": {},
   "source": [
    "## Training Dynamics and Convergence\n",
    "\n",
    "Visualize training progress and model convergence patterns for both models.\n",
    "\n",
    "**Why:** Training curves reveal whether models are learning effectively, overfitting, or underfitting. Convergence speed is important for practical deployment and understanding model efficiency.\n",
    "\n",
    "**What:**\n",
    "- Train/validation loss curves: Shows learning progress over epochs\n",
    "- Validation F1 progression: Entity-level metric trend\n",
    "- Comparison: Side-by-side BERT vs DeBERTa learning trajectories\n",
    "- Early stopping points: When each model stopped improving\n",
    "\n",
    "**How:** Plot epoch-wise metrics from training history stored during fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8b1b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model comparison\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MODEL COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create comparison table\n",
    "comparison_df = pd.DataFrame({\n",
    "    \"Metric\": [\"F1-Score\", \"Precision\", \"Recall\", \"Inference Time (s)\", \"Test Loss\"],\n",
    "    \"BERT\": [\n",
    "        f\"{bert_results['f1']:.4f}\",\n",
    "        f\"{bert_results['precision']:.4f}\",\n",
    "        f\"{bert_results['recall']:.4f}\",\n",
    "        f\"{bert_results['inference_time']:.2f}\",\n",
    "        f\"{bert_results['test_loss']:.4f}\"\n",
    "    ],\n",
    "    \"DeBERTa\": [\n",
    "        f\"{deberta_results['f1']:.4f}\",\n",
    "        f\"{deberta_results['precision']:.4f}\",\n",
    "        f\"{deberta_results['recall']:.4f}\",\n",
    "        f\"{deberta_results['inference_time']:.2f}\",\n",
    "        f\"{deberta_results['test_loss']:.4f}\"\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"\\nPerformance Metrics (Test Set)\")\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "# F1 and speed differences\n",
    "f1_diff = deberta_results['f1'] - bert_results['f1']\n",
    "f1_diff_pct = (f1_diff / bert_results['f1']) * 100 if bert_results['f1'] > 0 else 0\n",
    "speed_diff = deberta_results['inference_time'] / bert_results['inference_time']\n",
    "speed_diff_pct = (speed_diff - 1) * 100\n",
    "\n",
    "print(f\"\\nModel Differences:\")\n",
    "print(f\"  DeBERTa F1 vs BERT: {f1_diff:+.4f} ({f1_diff_pct:+.2f}%)\")\n",
    "print(f\"  DeBERTa Speed vs BERT: {speed_diff:.2f}x ({speed_diff_pct:+.2f}%)\")\n",
    "\n",
    "# Model architecture info\n",
    "print(f\"\\nModel Architecture:\")\n",
    "print(f\"  BERT: 110M parameters, 12 layers, 768 hidden dimensions\")\n",
    "print(f\"  DeBERTa: 86M parameters, 12 layers, 768 hidden dimensions\")\n",
    "print(f\"  → DeBERTa is ~22% smaller but potentially more efficient (disentangled attention)\")\n",
    "\n",
    "# Visualize F1 and speed comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# F1 comparison\n",
    "models = [\"BERT\", \"DeBERTa\"]\n",
    "f1_scores = [bert_results['f1'], deberta_results['f1']]\n",
    "colors = ['#1f77b4', '#ff7f0e']\n",
    "\n",
    "axes[0].bar(models, f1_scores, color=colors, alpha=0.7, edgecolor='black', linewidth=2)\n",
    "axes[0].set_ylabel('F1-Score', fontsize=12, fontweight='bold')\n",
    "axes[0].set_title('Overall F1-Score Comparison', fontsize=13, fontweight='bold')\n",
    "axes[0].set_ylim([0, 1])\n",
    "for i, (m, f1) in enumerate(zip(models, f1_scores)):\n",
    "    axes[0].text(i, f1 + 0.02, f'{f1:.4f}', ha='center', fontsize=11, fontweight='bold')\n",
    "\n",
    "# Inference time comparison\n",
    "inference_times = [bert_results['inference_time'], deberta_results['inference_time']]\n",
    "axes[1].bar(models, inference_times, color=colors, alpha=0.7, edgecolor='black', linewidth=2)\n",
    "axes[1].set_ylabel('Inference Time (seconds)', fontsize=12, fontweight='bold')\n",
    "axes[1].set_title('Inference Speed Comparison', fontsize=13, fontweight='bold')\n",
    "for i, (m, t) in enumerate(zip(models, inference_times)):\n",
    "    axes[1].text(i, t + 0.5, f'{t:.2f}s', ha='center', fontsize=11, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f317802a",
   "metadata": {},
   "source": [
    "## Model Comparison\n",
    "\n",
    "Compare BERT and DeBERTa on accuracy, speed, and computational efficiency.\n",
    "\n",
    "**Why:** Different models have different trade-offs. BERT is the baseline; DeBERTa is newer and smaller. Comparison reveals which is better for practical deployment.\n",
    "\n",
    "**What:**\n",
    "- F1-score: Overall and per-entity\n",
    "- Inference speed: Tokens per second\n",
    "- Model size: Parameter count\n",
    "- Training efficiency: Convergence speed\n",
    "- Trade-off analysis: Accuracy vs speed vs memory\n",
    "\n",
    "**How:** Create side-by-side comparison tables and bar charts for easy visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c74d641",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error analysis\n",
    "def analyze_errors(true_sequences, pred_sequences, model_name):\n",
    "    \"\"\"Analyze error patterns\"\"\"\n",
    "    print(f\"\\n{model_name} Error Analysis\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    error_counts = {}\n",
    "    \n",
    "    # Count errors\n",
    "    for true_seq, pred_seq in zip(true_sequences, pred_sequences):\n",
    "        for true_label, pred_label in zip(true_seq, pred_seq):\n",
    "            if true_label != pred_label:\n",
    "                key = (true_label, pred_label)\n",
    "                error_counts[key] = error_counts.get(key, 0) + 1\n",
    "    \n",
    "    # Display top errors\n",
    "    total_errors = sum(error_counts.values())\n",
    "    print(f\"\\nTotal errors: {total_errors}\")\n",
    "    print(f\"\\nTop 15 Error Types:\")\n",
    "    for (true_label, pred_label), count in sorted(error_counts.items(), key=lambda x: x[1], reverse=True)[:15]:\n",
    "        pct = 100 * count / total_errors\n",
    "        print(f\"  {true_label:>8} → {pred_label:>8}: {count:5d} times ({pct:5.2f}%)\")\n",
    "    \n",
    "    return error_counts\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ERROR ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "bert_error_counts = analyze_errors(\n",
    "    bert_results[\"labels\"], bert_results[\"predictions\"], \"BERT\"\n",
    ")\n",
    "\n",
    "deberta_error_counts = analyze_errors(\n",
    "    deberta_results[\"labels\"], deberta_results[\"predictions\"], \"DeBERTa\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cfd6d40",
   "metadata": {},
   "source": [
    "## Error Analysis\n",
    "\n",
    "Identify and analyze systematic errors in both models.\n",
    "\n",
    "**Why:** Understanding failure patterns is critical for improving models. Some errors are easier to fix (e.g., boundary detection) than others (e.g., semantic ambiguity).\n",
    "\n",
    "**What:**\n",
    "- Count misclassification types (which entity types cause the most errors)\n",
    "- Identify top error patterns by frequency\n",
    "- Compare error distributions between BERT and DeBERTa\n",
    "\n",
    "**How:** \n",
    "1. Find tokens where prediction ≠ ground truth\n",
    "2. Group by (true_label, pred_label) pair\n",
    "3. Count occurrences and display top errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2317c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate confusion matrices\n",
    "def plot_confusion_matrix(true_labels, pred_labels, model_name):\n",
    "    \"\"\"Plot confusion matrix for token-level predictions\"\"\"\n",
    "    # Flatten sequences\n",
    "    true_flat = [l for seq in true_labels for l in seq]\n",
    "    pred_flat = [p for seq in pred_labels for p in seq]\n",
    "    \n",
    "    # Create confusion matrix\n",
    "    cm = confusion_matrix(true_flat, pred_flat, labels=list(label2id.keys()))\n",
    "    \n",
    "    # Normalize by row (true label frequency)\n",
    "    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    \n",
    "    # Plot\n",
    "    fig, ax = plt.subplots(figsize=(12, 10))\n",
    "    sns.heatmap(cm_normalized, annot=True, fmt='.2f', cmap='Blues',\n",
    "                xticklabels=list(label2id.keys()),\n",
    "                yticklabels=list(label2id.keys()),\n",
    "                ax=ax, cbar_kws={'label': 'Proportion'})\n",
    "    ax.set_title(f'{model_name} - Confusion Matrix (Normalized)', fontsize=14, fontweight='bold')\n",
    "    ax.set_xlabel('Predicted Label', fontsize=12)\n",
    "    ax.set_ylabel('True Label', fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CONFUSION MATRICES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "plot_confusion_matrix(bert_results[\"labels\"], bert_results[\"predictions\"], \"BERT\")\n",
    "plot_confusion_matrix(deberta_results[\"labels\"], deberta_results[\"predictions\"], \"DeBERTa\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c36ddb",
   "metadata": {},
   "source": [
    "## Confusion Matrices\n",
    "\n",
    "Visualize which entity types are frequently confused or misclassified.\n",
    "\n",
    "**Why:** Confusion matrices reveal systematic errors (e.g., ORG frequently predicted as LOC). This guides model improvements.\n",
    "\n",
    "**What:**\n",
    "- Token-level confusion matrix: 9x9 grid showing confusion between all label pairs\n",
    "- Normalized by row: Shows proportion of each true label that's confused with other labels\n",
    "- Comparison: Side-by-side BERT vs DeBERTa confusion patterns\n",
    "\n",
    "**How:** \n",
    "1. Flatten predictions and ground truth labels\n",
    "2. Count (true_label, pred_label) pairs\n",
    "3. Normalize by row and visualize as heatmap using seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751496b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EVALUATING ON TEST SET\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "def evaluate_on_test(model, tokenizer, model_name, test_sents):\n",
    "    \"\"\"Full evaluation pipeline for test set\"\"\"\n",
    "    print(f\"\\n{model_name} Test Set Evaluation\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Tokenize test data\n",
    "    test_dataset = TokenizedNERDataset(test_sents, tokenizer, label2id)\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=TRAINING_CONFIG[\"batch_size\"],\n",
    "        shuffle=False,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "    \n",
    "    # Evaluate\n",
    "    start_time = time.time()\n",
    "    test_labels, test_preds, test_loss = evaluate(model, test_loader, device)\n",
    "    inference_time = time.time() - start_time\n",
    "    \n",
    "    # Convert to label strings for seqeval metrics\n",
    "    test_labels_strings = []\n",
    "    test_preds_strings = []\n",
    "    \n",
    "    for label_seq, pred_seq in zip(test_labels, test_preds):\n",
    "        label_strs = [id2label[l] for l in label_seq if l != -100]\n",
    "        pred_strs = [id2label[p] for i, p in enumerate(pred_seq) if label_seq[i] != -100]\n",
    "        \n",
    "        if label_strs:  # Only add non-empty sequences\n",
    "            test_labels_strings.append(label_strs)\n",
    "            test_preds_strings.append(pred_strs)\n",
    "    \n",
    "    # Compute metrics\n",
    "    precision = precision_score(test_labels_strings, test_preds_strings)\n",
    "    recall = recall_score(test_labels_strings, test_preds_strings)\n",
    "    f1 = f1_score(test_labels_strings, test_preds_strings)\n",
    "    \n",
    "    # Per-entity breakdown\n",
    "    print(\"\\nEntity-Level Metrics:\")\n",
    "    print(classification_report(test_labels_strings, test_preds_strings, digits=4))\n",
    "    print(f\"\\nOverall - Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}\")\n",
    "    print(f\"Inference time: {inference_time:.2f}s for {len(test_preds)} samples\")\n",
    "    \n",
    "    return {\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1,\n",
    "        \"test_loss\": test_loss,\n",
    "        \"inference_time\": inference_time,\n",
    "        \"labels\": test_labels_strings,\n",
    "        \"predictions\": test_preds_strings\n",
    "    }\n",
    "\n",
    "# Evaluate both models\n",
    "bert_results = evaluate_on_test(bert_model, bert_tokenizer, \"BERT\", test_sents)\n",
    "deberta_results = evaluate_on_test(deberta_model, deberta_tokenizer, \"DeBERTa\", test_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9f2288",
   "metadata": {},
   "source": [
    "## Evaluation on Test Set\n",
    "\n",
    "Evaluate both models on the final test set using entity-level and token-level metrics.\n",
    "\n",
    "**Why:** Test set evaluation is final assessment. Models never saw test data during training or validation.\n",
    "\n",
    "**What:**\n",
    "- Run inference on test set using best saved models\n",
    "- Compute entity-level precision, recall, F1 (using seqeval)\n",
    "- Generate per-entity breakdown (LOC, PER, ORG, MISC)\n",
    "- Measure inference time for speed comparison\n",
    "- Store predictions for error analysis and confusion matrices\n",
    "\n",
    "**Metrics:**\n",
    "- **Entity-level:** Entire entity must be correct (strict metric)\n",
    "  - Example: \"New York\" must be tagged [B-LOC, I-LOC] to count as correct\n",
    "- **Per-entity:** Separate scores for each entity type\n",
    "- **Inference time:** Wall-clock time to process all test samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb54ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tune DeBERTa\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINE-TUNING DeBERTa\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Tokenize data for DeBERTa\n",
    "print(\"\\nTokenizing data for DeBERTa...\")\n",
    "deberta_train_dataset = TokenizedNERDataset(train_sents, deberta_tokenizer, label2id)\n",
    "deberta_val_dataset = TokenizedNERDataset(val_sents, deberta_tokenizer, label2id)\n",
    "\n",
    "deberta_train_loader = DataLoader(\n",
    "    deberta_train_dataset,\n",
    "    batch_size=TRAINING_CONFIG[\"batch_size\"],\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "deberta_val_loader = DataLoader(\n",
    "    deberta_val_dataset,\n",
    "    batch_size=TRAINING_CONFIG[\"batch_size\"],\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "# Setup optimizer and scheduler\n",
    "deberta_optimizer = AdamW(deberta_model.parameters(), lr=TRAINING_CONFIG[\"learning_rate\"])\n",
    "deberta_scheduler = get_linear_schedule_with_warmup(\n",
    "    deberta_optimizer,\n",
    "    num_warmup_steps=TRAINING_CONFIG[\"warmup_steps\"],\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "# Training loop with early stopping\n",
    "deberta_best_f1 = -1.0\n",
    "deberta_patience_counter = 0\n",
    "deberta_history = {\"train_loss\": [], \"val_loss\": [], \"val_f1\": []}\n",
    "\n",
    "for epoch in range(TRAINING_CONFIG[\"num_epochs\"]):\n",
    "    print(f\"\\nEpoch {epoch+1}/{TRAINING_CONFIG['num_epochs']}\")\n",
    "    \n",
    "    # Train\n",
    "    deberta_train_loss = train_epoch(deberta_model, deberta_train_loader, deberta_optimizer, deberta_scheduler, device)\n",
    "    deberta_history[\"train_loss\"].append(deberta_train_loss)\n",
    "    print(f\"  Train loss: {deberta_train_loss:.4f}\")\n",
    "    \n",
    "    # Validate\n",
    "    deberta_val_labels, deberta_val_preds, deberta_val_loss = evaluate(deberta_model, deberta_val_loader, device)\n",
    "    deberta_history[\"val_loss\"].append(deberta_val_loss)\n",
    "    print(f\"  Val loss: {deberta_val_loss:.4f}\")\n",
    "    \n",
    "    # Convert to label strings for seqeval\n",
    "    deberta_val_labels_seqeval = [\n",
    "        [id2label[l] for l in seq if l != -100]\n",
    "        for seq in deberta_val_labels\n",
    "    ]\n",
    "    deberta_val_preds_seqeval = [\n",
    "        [id2label[p] for i, p in enumerate(seq) if deberta_val_labels[j][i] != -100]\n",
    "        for j, seq in enumerate(deberta_val_preds)\n",
    "    ]\n",
    "    \n",
    "    # Compute F1\n",
    "    try:\n",
    "        deberta_val_f1 = f1_score(deberta_val_labels_seqeval, deberta_val_preds_seqeval)\n",
    "    except:\n",
    "        deberta_val_f1 = -1.0\n",
    "    \n",
    "    deberta_history[\"val_f1\"].append(deberta_val_f1)\n",
    "    print(f\"  Val F1: {deberta_val_f1:.4f}\")\n",
    "    \n",
    "    # Early stopping\n",
    "    if deberta_val_f1 > deberta_best_f1:\n",
    "        deberta_best_f1 = deberta_val_f1\n",
    "        deberta_patience_counter = 0\n",
    "        torch.save(deberta_model.state_dict(), \"deberta_best.pt\")\n",
    "        print(f\"  ✓ Best model saved (F1: {deberta_best_f1:.4f})\")\n",
    "    else:\n",
    "        deberta_patience_counter += 1\n",
    "        if deberta_patience_counter >= TRAINING_CONFIG[\"patience\"]:\n",
    "            print(f\"  Early stopping (patience {TRAINING_CONFIG['patience']} reached)\")\n",
    "            break\n",
    "\n",
    "# Load best model\n",
    "deberta_model.load_state_dict(torch.load(\"deberta_best.pt\"))\n",
    "print(f\"\\nDeBERTa Best validation F1: {deberta_best_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9889003c",
   "metadata": {},
   "source": [
    "## Fine-tune DeBERTa Model\n",
    "\n",
    "Fine-tune DeBERTa on the CoNLL-2003 NER dataset with same methodology as BERT for fair comparison.\n",
    "\n",
    "**What happens:**\n",
    "1. Tokenize training and validation data using DeBERTa tokenizer\n",
    "2. Create DataLoaders with identical batching configuration\n",
    "3. Set up optimizer and scheduler with same hyperparameters\n",
    "4. Train with same early stopping criteria as BERT\n",
    "\n",
    "**Why same config:**\n",
    "- Ensures fair comparison between architectures\n",
    "- Only differences are model size and attention mechanism\n",
    "- Hyperparameter tuning is not the focus; architectural comparison is\n",
    "\n",
    "**Output:**\n",
    "- Best model saved to `deberta_best.pt`\n",
    "- Training history for comparison with BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b45452",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tune BERT\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINE-TUNING BERT\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Tokenize data for BERT\n",
    "print(\"\\nTokenizing data for BERT...\")\n",
    "bert_train_dataset = TokenizedNERDataset(train_sents, bert_tokenizer, label2id)\n",
    "bert_val_dataset = TokenizedNERDataset(val_sents, bert_tokenizer, label2id)\n",
    "\n",
    "bert_train_loader = DataLoader(\n",
    "    bert_train_dataset,\n",
    "    batch_size=TRAINING_CONFIG[\"batch_size\"],\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "bert_val_loader = DataLoader(\n",
    "    bert_val_dataset,\n",
    "    batch_size=TRAINING_CONFIG[\"batch_size\"],\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "# Setup optimizer and scheduler\n",
    "bert_optimizer = AdamW(bert_model.parameters(), lr=TRAINING_CONFIG[\"learning_rate\"])\n",
    "total_steps = len(bert_train_loader) * TRAINING_CONFIG[\"num_epochs\"] // TRAINING_CONFIG[\"accumulation_steps\"]\n",
    "bert_scheduler = get_linear_schedule_with_warmup(\n",
    "    bert_optimizer,\n",
    "    num_warmup_steps=TRAINING_CONFIG[\"warmup_steps\"],\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "# Training loop with early stopping\n",
    "bert_best_f1 = -1.0\n",
    "bert_patience_counter = 0\n",
    "bert_history = {\"train_loss\": [], \"val_loss\": [], \"val_f1\": []}\n",
    "\n",
    "for epoch in range(TRAINING_CONFIG[\"num_epochs\"]):\n",
    "    print(f\"\\nEpoch {epoch+1}/{TRAINING_CONFIG['num_epochs']}\")\n",
    "    \n",
    "    # Train\n",
    "    bert_train_loss = train_epoch(bert_model, bert_train_loader, bert_optimizer, bert_scheduler, device)\n",
    "    bert_history[\"train_loss\"].append(bert_train_loss)\n",
    "    print(f\"  Train loss: {bert_train_loss:.4f}\")\n",
    "    \n",
    "    # Validate\n",
    "    bert_val_labels, bert_val_preds, bert_val_loss = evaluate(bert_model, bert_val_loader, device)\n",
    "    bert_history[\"val_loss\"].append(bert_val_loss)\n",
    "    print(f\"  Val loss: {bert_val_loss:.4f}\")\n",
    "    \n",
    "    # Convert to label strings for seqeval\n",
    "    bert_val_labels_seqeval = [\n",
    "        [id2label[l] for l in seq if l != -100]\n",
    "        for seq in bert_val_labels\n",
    "    ]\n",
    "    bert_val_preds_seqeval = [\n",
    "        [id2label[p] for i, p in enumerate(seq) if bert_val_labels[j][i] != -100]\n",
    "        for j, seq in enumerate(bert_val_preds)\n",
    "    ]\n",
    "    \n",
    "    # Compute F1\n",
    "    try:\n",
    "        bert_val_f1 = f1_score(bert_val_labels_seqeval, bert_val_preds_seqeval)\n",
    "    except:\n",
    "        bert_val_f1 = -1.0\n",
    "    \n",
    "    bert_history[\"val_f1\"].append(bert_val_f1)\n",
    "    print(f\"  Val F1: {bert_val_f1:.4f}\")\n",
    "    \n",
    "    # Early stopping\n",
    "    if bert_val_f1 > bert_best_f1:\n",
    "        bert_best_f1 = bert_val_f1\n",
    "        bert_patience_counter = 0\n",
    "        torch.save(bert_model.state_dict(), \"bert_best.pt\")\n",
    "        print(f\"  ✓ Best model saved (F1: {bert_best_f1:.4f})\")\n",
    "    else:\n",
    "        bert_patience_counter += 1\n",
    "        if bert_patience_counter >= TRAINING_CONFIG[\"patience\"]:\n",
    "            print(f\"  Early stopping (patience {TRAINING_CONFIG['patience']} reached)\")\n",
    "            break\n",
    "\n",
    "# Load best model\n",
    "bert_model.load_state_dict(torch.load(\"bert_best.pt\"))\n",
    "print(f\"\\nBERT Best validation F1: {bert_best_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d608c9",
   "metadata": {},
   "source": [
    "## Fine-tune BERT Model\n",
    "\n",
    "Fine-tune BERT on the CoNLL-2003 NER dataset with validation-based early stopping.\n",
    "\n",
    "**What happens:**\n",
    "1. Tokenize training and validation data using BERT tokenizer\n",
    "2. Create DataLoaders with batching and padding\n",
    "3. Set up AdamW optimizer and learning rate scheduler with warmup\n",
    "4. For each epoch:\n",
    "   - Train on training set with gradient accumulation and clipping\n",
    "   - Validate on validation set\n",
    "   - Compute entity-level F1 score\n",
    "   - Save checkpoint if F1 improves\n",
    "   - Early stop if no improvement for 3 epochs\n",
    "\n",
    "**Output:**\n",
    "- Best model saved to `bert_best.pt`\n",
    "- Training history (loss and F1 curves)\n",
    "- Final best validation F1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf7e7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "TRAINING_CONFIG = {\n",
    "    \"learning_rate\": 2e-5,\n",
    "    \"batch_size\": 16,\n",
    "    \"accumulation_steps\": 2,\n",
    "    \"num_epochs\": 5,\n",
    "    \"warmup_steps\": 500,\n",
    "    \"max_grad_norm\": 1.0,\n",
    "    \"patience\": 3  # early stopping patience\n",
    "}\n",
    "\n",
    "def train_epoch(model, train_loader, optimizer, scheduler, device):\n",
    "    \"\"\"Train for one epoch with gradient accumulation\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for step, batch in enumerate(tqdm(train_loader, desc=\"Training\", leave=False)):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels\n",
    "        )\n",
    "        loss = outputs.loss\n",
    "        \n",
    "        # Gradient accumulation\n",
    "        loss = loss / TRAINING_CONFIG[\"accumulation_steps\"]\n",
    "        loss.backward()\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        if (step + 1) % TRAINING_CONFIG[\"accumulation_steps\"] == 0:\n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(\n",
    "                model.parameters(),\n",
    "                TRAINING_CONFIG[\"max_grad_norm\"]\n",
    "            )\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "    \n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "\n",
    "def evaluate(model, eval_loader, device):\n",
    "    \"\"\"Evaluate model on validation/test set\"\"\"\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "    total_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(eval_loader, desc=\"Evaluating\", leave=False):\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            \n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels\n",
    "            )\n",
    "            \n",
    "            loss = outputs.loss\n",
    "            logits = outputs.logits\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Get predictions\n",
    "            predictions = logits.argmax(dim=-1).cpu().tolist()\n",
    "            labels_list = labels.cpu().tolist()\n",
    "            \n",
    "            # Store for later evaluation\n",
    "            for pred_seq, label_seq in zip(predictions, labels_list):\n",
    "                all_predictions.append(pred_seq)\n",
    "                all_labels.append(label_seq)\n",
    "    \n",
    "    return all_labels, all_predictions, total_loss / len(eval_loader)\n",
    "\n",
    "print(\"Training utilities created successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685313c5",
   "metadata": {},
   "source": [
    "## Training Configuration and Utilities\n",
    "\n",
    "Set up training infrastructure with gradient clipping, learning rate scheduling, and early stopping.\n",
    "\n",
    "**Why:** Effective fine-tuning of transformers requires:\n",
    "- Appropriate learning rate (LR) to avoid catastrophic forgetting of pretrained weights\n",
    "- Gradient clipping to prevent exploding gradients\n",
    "- Learning rate scheduling (warmup then linear decay) for convergence\n",
    "- Validation-based early stopping to prevent overfitting\n",
    "\n",
    "**What:**\n",
    "- `train_epoch()`: Train for one epoch with gradient accumulation\n",
    "- `evaluate()`: Run inference on validation/test set, compute metrics\n",
    "- Early stopping: Stop if validation F1 doesn't improve for N epochs\n",
    "- Warmup scheduler: Gradual LR increase, then linear decay\n",
    "\n",
    "**Hyperparameters:**\n",
    "- Learning rate: 2e-5 (conservative, prevents catastrophic forgetting)\n",
    "- Batch size: 16 (per-device), gradient accumulation: 2x (→ effective batch size 32)\n",
    "- Max gradient norm: 1.0 (clip to prevent exploding gradients)\n",
    "- Number of epochs: 5\n",
    "- Early stopping patience: 3 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4510525f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configurations\n",
    "MODEL_CONFIGS = {\n",
    "    \"bert\": {\n",
    "        \"model_name\": \"bert-base-cased\",\n",
    "        \"num_labels\": len(label2id),\n",
    "        \"id2label\": id2label,\n",
    "        \"label2id\": label2id,\n",
    "    },\n",
    "    \"deberta\": {\n",
    "        \"model_name\": \"microsoft/deberta-v3-base\",\n",
    "        \"num_labels\": len(label2id),\n",
    "        \"id2label\": id2label,\n",
    "        \"label2id\": label2id,\n",
    "    }\n",
    "}\n",
    "\n",
    "def load_model_and_tokenizer(model_key):\n",
    "    \"\"\"Load transformer model and tokenizer for token classification\"\"\"\n",
    "    config = MODEL_CONFIGS[model_key]\n",
    "    model_name = config[\"model_name\"]\n",
    "    \n",
    "    print(f\"\\nLoading {model_key.upper()} ({model_name})...\")\n",
    "    \n",
    "    # Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "    # Load model for token classification\n",
    "    model = AutoModelForTokenClassification.from_pretrained(\n",
    "        model_name,\n",
    "        num_labels=config[\"num_labels\"],\n",
    "        id2label=config[\"id2label\"],\n",
    "        label2id=config[\"label2id\"],\n",
    "        ignore_mismatched_sizes=False\n",
    "    )\n",
    "    \n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Count parameters\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    \n",
    "    print(f\"  Total parameters: {total_params:,}\")\n",
    "    print(f\"  Trainable parameters: {trainable_params:,}\")\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "# Load both models\n",
    "print(\"=\" * 60)\n",
    "print(\"LOADING TRANSFORMER MODELS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "bert_model, bert_tokenizer = load_model_and_tokenizer(\"bert\")\n",
    "deberta_model, deberta_tokenizer = load_model_and_tokenizer(\"deberta\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce5f605",
   "metadata": {},
   "source": [
    "## Model Selection and Loading\n",
    "\n",
    "Select and load two different transformer-based models for token classification.\n",
    "\n",
    "**Why:** Different transformer architectures have different pretraining objectives, parameter efficiency, and performance characteristics. Comparing them reveals which is better suited for NER.\n",
    "\n",
    "**What:**\n",
    "- **Model 1: BERT (bert-base-cased)** - Classic transformer baseline, bidirectional context, 12 layers, 110M parameters\n",
    "- **Model 2: DeBERTa (deberta-v3-base)** - Modern transformer variant with disentangled attention, 12 layers, 86M parameters (more efficient)\n",
    "\n",
    "Both loaded with AutoModelForTokenClassification for the NER task (9 labels for CoNLL-2003).\n",
    "\n",
    "**How:** Load pretrained weights and modify the classification head to output 9 labels (one per tag type)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087a71ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization and dataset class\n",
    "class TokenizedNERDataset(Dataset):\n",
    "    \"\"\"PyTorch dataset for tokenized NER data\"\"\"\n",
    "    def __init__(self, sentences, tokenizer, label2id, max_length=512):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            sentences: list of (tokens, pos_tags, labels)\n",
    "            tokenizer: from transformers (e.g., AutoTokenizer)\n",
    "            label2id: dict mapping label strings to IDs\n",
    "            max_length: maximum sequence length\n",
    "        \"\"\"\n",
    "        self.tokenizer = tokenizer\n",
    "        self.label2id = label2id\n",
    "        self.max_length = max_length\n",
    "        self.encoded_data = []\n",
    "        \n",
    "        for tokens, _, labels in sentences:\n",
    "            # Tokenize with word_ids tracking\n",
    "            encoding = tokenizer(\n",
    "                tokens,\n",
    "                truncation=True,\n",
    "                max_length=max_length,\n",
    "                padding='max_length',\n",
    "                is_split_into_words=True,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "            \n",
    "            # Align labels to subword tokens\n",
    "            word_ids = encoding.word_ids()\n",
    "            label_ids = [-100] * len(word_ids)  # -100 = ignore in loss\n",
    "            \n",
    "            for idx, word_id in enumerate(word_ids):\n",
    "                if word_id is not None:\n",
    "                    # Only assign label to first subword of each word\n",
    "                    if idx == 0 or encoding.word_ids()[idx-1] != word_id:\n",
    "                        label_ids[idx] = label2id.get(labels[word_id], label2id.get(\"O\", 0))\n",
    "            \n",
    "            self.encoded_data.append({\n",
    "                \"input_ids\": encoding[\"input_ids\"].squeeze(),\n",
    "                \"attention_mask\": encoding[\"attention_mask\"].squeeze(),\n",
    "                \"labels\": torch.tensor(label_ids, dtype=torch.long)\n",
    "            })\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.encoded_data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.encoded_data[idx]\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"Collate batch for DataLoader\"\"\"\n",
    "    input_ids = torch.stack([item[\"input_ids\"] for item in batch])\n",
    "    attention_mask = torch.stack([item[\"attention_mask\"] for item in batch])\n",
    "    labels = torch.stack([item[\"labels\"] for item in batch])\n",
    "    \n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels\": labels\n",
    "    }\n",
    "\n",
    "print(\"Dataset class created successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4ec25f",
   "metadata": {},
   "source": [
    "## Tokenization and Dataset Class\n",
    "\n",
    "Create a PyTorch dataset that handles subword token alignment for transformer models.\n",
    "\n",
    "**Why:** Transformers tokenize text differently than word tokenizers. \"New York\" might become [\"New\", \"Y\", \"##ork\"], requiring label alignment.\n",
    "\n",
    "**What:**\n",
    "- `TokenizedNERDataset`: PyTorch dataset that tokenizes sentences and aligns labels with subword tokens\n",
    "- `word_ids()`: Method that maps subword tokens back to original words for evaluation\n",
    "- Batch collation: Pads sequences to max length in batch\n",
    "\n",
    "**How:** For each subword token created by the transformer tokenizer, check if it's part of a word. First subword of a word gets the word's label; continuation subwords get ignored (-100) during loss computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44822826",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CoNLL-2003 from Assignment 2\n",
    "cwd = Path.cwd()\n",
    "local_dir = Path(\"Assignment2_Name_Entity_Recognition/conll2003\")\n",
    "alt_dir = Path(\"conll2003\")\n",
    "parent_dir = Path(\"../Assignment2_Name_Entity_Recognition/conll2003\")\n",
    "\n",
    "if (cwd / local_dir).exists():\n",
    "    data_dir = cwd / local_dir\n",
    "elif (cwd / alt_dir).exists():\n",
    "    data_dir = cwd / alt_dir\n",
    "elif (cwd / parent_dir).exists():\n",
    "    data_dir = cwd / parent_dir\n",
    "else:\n",
    "    raise FileNotFoundError(\"Cannot find conll2003 folder\")\n",
    "\n",
    "train_path = data_dir / \"eng.train\"\n",
    "val_path = data_dir / \"eng.testa\"\n",
    "test_path = data_dir / \"eng.testb\"\n",
    "\n",
    "# Parse CoNLL format (same as Assignment 2)\n",
    "def read_conll(path):\n",
    "    \"\"\"Parse CoNLL-2003 format file\"\"\"\n",
    "    sents = []\n",
    "    tokens = []\n",
    "    pos_tags = []\n",
    "    labels = []\n",
    "    with path.open(\"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                if tokens:\n",
    "                    sents.append((tokens, pos_tags, labels))\n",
    "                    tokens, pos_tags, labels = [], [], []\n",
    "                continue\n",
    "            if line.startswith(\"-DOCSTART-\"):\n",
    "                continue\n",
    "            parts = line.split()\n",
    "            if len(parts) < 4:\n",
    "                continue\n",
    "            token, pos, _chunk, ner = parts[0], parts[1], parts[2], parts[3]\n",
    "            tokens.append(token)\n",
    "            pos_tags.append(pos)\n",
    "            labels.append(ner)\n",
    "    if tokens:\n",
    "        sents.append((tokens, pos_tags, labels))\n",
    "    return sents\n",
    "\n",
    "# Load datasets\n",
    "train_sents = read_conll(train_path)\n",
    "val_sents = read_conll(val_path)\n",
    "test_sents = read_conll(test_path)\n",
    "\n",
    "# Extract all labels and create label mappings\n",
    "label_set = set()\n",
    "for _, _, labels in train_sents:\n",
    "    label_set.update(labels)\n",
    "label2id = {label: idx for idx, label in enumerate(sorted(label_set))}\n",
    "id2label = {idx: label for label, idx in label2id.items()}\n",
    "\n",
    "print(f\"Dataset: CoNLL-2003\")\n",
    "print(f\"Labels: {list(label2id.keys())}\")\n",
    "print(f\"Train: {len(train_sents)}, Val: {len(val_sents)}, Test: {len(test_sents)}\")\n",
    "print(f\"Label mapping: {label2id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81d64c9",
   "metadata": {},
   "source": [
    "## Data Loading and Preparation\n",
    "\n",
    "Load the CoNLL-2003 dataset from Assignment 2 and prepare it for transformer models.\n",
    "\n",
    "**Why:** Transformers use subword tokenization (different from Assignment 2's word-level tokens). We must align BIO labels with subword tokens.\n",
    "\n",
    "**What:**\n",
    "- `read_conll()`: Parse CoNLL-2003 files\n",
    "- Extract tokens, POS tags, and labels\n",
    "- Build label vocabulary\n",
    "- Store in format ready for transformer tokenization\n",
    "\n",
    "**How:** Read CoNLL format files where each line has: token POS chunk NER-tag. Blank lines separate sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758ac197",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from pathlib import Path\n",
    "import time\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim import AdamW\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForTokenClassification,\n",
    "    get_linear_schedule_with_warmup, set_seed\n",
    ")\n",
    "from seqeval.metrics import classification_report, f1_score, precision_score, recall_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "set_seed(42)\n",
    "\n",
    "# Device setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"Running on CPU (GPU recommended for faster training)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6fa77f",
   "metadata": {},
   "source": [
    "## Setup: Imports and Configuration\n",
    "\n",
    "Load required libraries and set random seeds for reproducibility.\n",
    "\n",
    "**Why:** Transformers use random initialization. Fix seeds to ensure reproducible results across runs.\n",
    "\n",
    "**What:** Import:\n",
    "- `transformers`: For model loading, tokenizers, and training utilities\n",
    "- `torch`: For GPU/CPU detection and model operations\n",
    "- `seqeval.metrics`: For entity-level NER evaluation\n",
    "- `sklearn`: For confusion matrices and classification metrics\n",
    "- `tqdm`: For progress tracking\n",
    "\n",
    "**How:** Set random seeds (42) for Python, NumPy, PyTorch, and Hugging Face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc4fa75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (run once)\n",
    "!pip -q install transformers torch datasets seqeval scikit-learn matplotlib seaborn pandas numpy tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3db65b",
   "metadata": {},
   "source": [
    "## Setup: Dependencies\n",
    "\n",
    "Install required packages for transformer models, NER evaluation, and visualization.\n",
    "\n",
    "**Why:** This assignment uses Hugging Face transformers for model access, PyTorch for training, and additional tools for evaluation and visualization.\n",
    "\n",
    "**What:** Install packages for:\n",
    "- `transformers` and `datasets`: Hugging Face ecosystem for pretrained models and data handling\n",
    "- `torch`: Neural network framework\n",
    "- `seqeval`: Entity-level NER metrics\n",
    "- `scikit-learn`: For confusion matrices and metrics\n",
    "- `matplotlib` and `seaborn`: For visualization\n",
    "\n",
    "**Note:** Run this once at the beginning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74e2f2b",
   "metadata": {},
   "source": [
    "# Assignment 3: Transformer-Based NER Models\n",
    "\n",
    "This notebook fine-tunes and compares two transformer-based models (BERT and DeBERTa) for Named Entity Recognition on the CoNLL-2003 dataset.\n",
    "\n",
    "**Goals:**\n",
    "- Fine-tune two different transformer architectures for token classification\n",
    "- Evaluate performance using entity-level and token-level metrics\n",
    "- Analyze misclassified entities and identify systematic errors\n",
    "- Compare models on accuracy, speed, and computational efficiency\n",
    "- Understand how pretraining impacts NER performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38918a8",
   "metadata": {},
   "source": [
    "## Summary and Next Steps\n",
    "\n",
    "Key findings from transformer model comparison:\n",
    "\n",
    "**Notebook Outputs:**\n",
    "- Both models were fine-tuned for 5 epochs with early stopping (patience=3)\n",
    "- Entity-level F1 scores reported using seqeval library\n",
    "- Confusion matrices show most frequent misclassifications\n",
    "- Error analysis identifies systematic failure patterns\n",
    "\n",
    "**For the Report:**\n",
    "See `MingHsiangLee_Assignment3_report.md` for:\n",
    "1. Detailed performance analysis\n",
    "2. Trade-off discussion (accuracy vs speed vs model size)\n",
    "3. Impact of pretraining on NER performance\n",
    "4. Limitations and potential improvements\n",
    "5. Concrete error examples and explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ad701e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Training dynamics visualization\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m fig, axes = \u001b[43mplt\u001b[49m.subplots(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m, figsize=(\u001b[32m14\u001b[39m, \u001b[32m5\u001b[39m))\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Training and validation loss\u001b[39;00m\n\u001b[32m      5\u001b[39m epochs_bert = \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(bert_history[\u001b[33m\"\u001b[39m\u001b[33mtrain_loss\u001b[39m\u001b[33m\"\u001b[39m]) + \u001b[32m1\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "# Training dynamics visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Training and validation loss\n",
    "epochs_bert = range(1, len(bert_history[\"train_loss\"]) + 1)\n",
    "epochs_deberta = range(1, len(deberta_history[\"train_loss\"]) + 1)\n",
    "\n",
    "axes[0].plot(epochs_bert, bert_history[\"train_loss\"], label=\"BERT Train Loss\", marker='o', linewidth=2)\n",
    "axes[0].plot(epochs_bert, bert_history[\"val_loss\"], label=\"BERT Val Loss\", marker='s', linewidth=2)\n",
    "axes[0].plot(epochs_deberta, deberta_history[\"train_loss\"], label=\"DeBERTa Train Loss\", marker='o', linewidth=2, linestyle='--')\n",
    "axes[0].plot(epochs_deberta, deberta_history[\"val_loss\"], label=\"DeBERTa Val Loss\", marker='s', linewidth=2, linestyle='--')\n",
    "axes[0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0].set_ylabel('Loss', fontsize=12)\n",
    "axes[0].set_title('Training and Validation Loss', fontsize=13, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Validation F1\n",
    "axes[1].plot(epochs_bert, bert_history[\"val_f1\"], label=\"BERT\", marker='o', linewidth=2.5, markersize=8)\n",
    "axes[1].plot(epochs_deberta, deberta_history[\"val_f1\"], label=\"DeBERTa\", marker='s', linewidth=2.5, markersize=8)\n",
    "axes[1].set_xlabel('Epoch', fontsize=12)\n",
    "axes[1].set_ylabel('Validation F1-Score', fontsize=12)\n",
    "axes[1].set_title('Validation F1 Progression', fontsize=13, fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTraining Summary:\")\n",
    "print(f\"BERT: Started at F1 {bert_history['val_f1'][0]:.4f}, ended at {bert_history['val_f1'][-1]:.4f}\")\n",
    "print(f\"DeBERTa: Started at F1 {deberta_history['val_f1'][0]:.4f}, ended at {deberta_history['val_f1'][-1]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bdf481c",
   "metadata": {},
   "source": [
    "## Training Dynamics and Convergence\n",
    "\n",
    "Visualize training progress and model convergence patterns.\n",
    "\n",
    "**Why:** Training curves reveal whether models are learning effectively, overfitting, or underfitting. Convergence speed is important for practical deployment.\n",
    "\n",
    "**What:**\n",
    "- Train/validation loss curves\n",
    "- Validation F1 progression\n",
    "- Comparison of learning trajectories between BERT and DeBERTa\n",
    "\n",
    "**How:** Plot epoch-wise metrics from training history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28efe9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model comparison\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MODEL COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create comparison table\n",
    "comparison_df = pd.DataFrame({\n",
    "    \"Metric\": [\"F1-Score\", \"Precision\", \"Recall\", \"Inference Time (s)\", \"Tokens/sec\"],\n",
    "    \"BERT\": [\n",
    "        f\"{bert_results['f1']:.4f}\",\n",
    "        f\"{bert_results['precision']:.4f}\",\n",
    "        f\"{bert_results['recall']:.4f}\",\n",
    "        f\"{bert_results['inference_time']:.2f}\",\n",
    "        f\"{len(test_sents) * 500 / bert_results['inference_time']:.0f}\"  # Approx tokens\n",
    "    ],\n",
    "    \"DeBERTa\": [\n",
    "        f\"{deberta_results['f1']:.4f}\",\n",
    "        f\"{deberta_results['precision']:.4f}\",\n",
    "        f\"{deberta_results['recall']:.4f}\",\n",
    "        f\"{deberta_results['inference_time']:.2f}\",\n",
    "        f\"{len(test_sents) * 500 / deberta_results['inference_time']:.0f}\"\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"\\nPerformance Metrics (Test Set)\")\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "# F1 difference\n",
    "f1_diff = (deberta_results['f1'] - bert_results['f1']) * 100\n",
    "speed_diff = (deberta_results['inference_time'] / bert_results['inference_time'] - 1) * 100\n",
    "\n",
    "print(f\"\\nModel Differences:\")\n",
    "print(f\"  DeBERTa F1 vs BERT: {f1_diff:+.2f}%\")\n",
    "print(f\"  DeBERTa Speed vs BERT: {speed_diff:+.2f}%\")\n",
    "\n",
    "# Note: Model size information\n",
    "print(f\"\\nModel Architecture:\")\n",
    "print(f\"  BERT: 110M parameters, 12 layers, 768 hidden\")\n",
    "print(f\"  DeBERTa: 86M parameters, 12 layers, 768 hidden\")\n",
    "print(f\"  → DeBERTa is ~22% smaller but potentially more efficient\")\n",
    "\n",
    "# Visualize F1 comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# F1 comparison\n",
    "models = [\"BERT\", \"DeBERTa\"]\n",
    "f1_scores = [bert_results['f1'], deberta_results['f1']]\n",
    "colors = ['#1f77b4', '#ff7f0e']\n",
    "\n",
    "axes[0].bar(models, f1_scores, color=colors, alpha=0.7, edgecolor='black')\n",
    "axes[0].set_ylabel('F1-Score', fontsize=12)\n",
    "axes[0].set_title('Overall F1-Score Comparison', fontsize=13, fontweight='bold')\n",
    "axes[0].set_ylim([0, 1])\n",
    "for i, (m, f1) in enumerate(zip(models, f1_scores)):\n",
    "    axes[0].text(i, f1 + 0.02, f'{f1:.4f}', ha='center', fontsize=11, fontweight='bold')\n",
    "\n",
    "# Inference time comparison\n",
    "inference_times = [bert_results['inference_time'], deberta_results['inference_time']]\n",
    "axes[1].bar(models, inference_times, color=colors, alpha=0.7, edgecolor='black')\n",
    "axes[1].set_ylabel('Inference Time (seconds)', fontsize=12)\n",
    "axes[1].set_title('Inference Speed Comparison', fontsize=13, fontweight='bold')\n",
    "for i, (m, t) in enumerate(zip(models, inference_times)):\n",
    "    axes[1].text(i, t + 0.1, f'{t:.2f}s', ha='center', fontsize=11, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9498ca37",
   "metadata": {},
   "source": [
    "## Model Comparison\n",
    "\n",
    "Compare BERT and DeBERTa on accuracy, speed, and computational efficiency.\n",
    "\n",
    "**Why:** Different models have different trade-offs. BERT is the baseline; DeBERTa is newer and smaller. Comparison reveals which is better for practical deployment.\n",
    "\n",
    "**What:**\n",
    "- F1-score: Overall and per-entity\n",
    "- Inference speed: Tokens per second\n",
    "- Model size: Parameter count\n",
    "- Training efficiency: Time to convergence\n",
    "\n",
    "**How:** Create side-by-side tables and bar charts for easy comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b558ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error analysis\n",
    "def analyze_errors(true_sequences, pred_sequences, test_sentences_raw, model_name):\n",
    "    \"\"\"Analyze error patterns\"\"\"\n",
    "    print(f\"\\n{model_name} Error Analysis\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    error_counts = {}\n",
    "    error_examples = {}\n",
    "    \n",
    "    # Count errors\n",
    "    for true_seq, pred_seq, (tokens, _, _) in zip(true_sequences, pred_sequences, test_sentences_raw):\n",
    "        for true_label, pred_label in zip(true_seq, pred_seq):\n",
    "            if true_label != pred_label:\n",
    "                key = (true_label, pred_label)\n",
    "                error_counts[key] = error_counts.get(key, 0) + 1\n",
    "                \n",
    "                if key not in error_examples:\n",
    "                    error_examples[key] = []\n",
    "                if len(error_examples[key]) < 3:  # Store first 3 examples\n",
    "                    error_examples[key].append((true_label, pred_label, tokens[:20]))  # First 20 tokens for context\n",
    "    \n",
    "    # Display top errors\n",
    "    print(f\"\\nTop 10 Error Types (Total errors: {sum(error_counts.values())}):\")\n",
    "    for (true_label, pred_label), count in sorted(error_counts.items(), key=lambda x: x[1], reverse=True)[:10]:\n",
    "        print(f\"  {true_label} → {pred_label}: {count} times\")\n",
    "    \n",
    "    return error_counts, error_examples\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ERROR ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "bert_error_counts, bert_error_examples = analyze_errors(\n",
    "    bert_results[\"labels\"], bert_results[\"predictions\"], test_sents, \"BERT\"\n",
    ")\n",
    "\n",
    "deberta_error_counts, deberta_error_examples = analyze_errors(\n",
    "    deberta_results[\"labels\"], deberta_results[\"predictions\"], test_sents, \"DeBERTa\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75832e2",
   "metadata": {},
   "source": [
    "## Error Analysis\n",
    "\n",
    "Identify and analyze systematic errors in both models.\n",
    "\n",
    "**Why:** Understanding failure patterns is critical for improving models. Some errors are easier to fix (e.g., boundary detection) than others (e.g., semantic ambiguity).\n",
    "\n",
    "**What:**\n",
    "- Count misclassification types (which entity types cause the most errors)\n",
    "- Identify examples of each error type\n",
    "- Compare error patterns between BERT and DeBERTa\n",
    "\n",
    "**How:** \n",
    "1. Find tokens where prediction ≠ ground truth\n",
    "2. Group by (true_label, pred_label) pair\n",
    "3. Extract representative examples for each common error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a8e28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate confusion matrices\n",
    "def plot_confusion_matrix(true_labels, pred_labels, model_name):\n",
    "    \"\"\"Plot confusion matrix for token-level predictions\"\"\"\n",
    "    # Flatten sequences\n",
    "    true_flat = [l for seq in true_labels for l in seq]\n",
    "    pred_flat = [p for seq in pred_labels for p in seq]\n",
    "    \n",
    "    # Create confusion matrix\n",
    "    cm = confusion_matrix(true_flat, pred_flat, labels=list(label2id.keys()))\n",
    "    \n",
    "    # Normalize by row (true label frequency)\n",
    "    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    \n",
    "    # Plot\n",
    "    fig, ax = plt.subplots(figsize=(12, 10))\n",
    "    sns.heatmap(cm_normalized, annot=True, fmt='.2f', cmap='Blues',\n",
    "                xticklabels=list(label2id.keys()),\n",
    "                yticklabels=list(label2id.keys()),\n",
    "                ax=ax, cbar_kws={'label': 'Proportion'})\n",
    "    ax.set_title(f'{model_name} - Confusion Matrix (Normalized)')\n",
    "    ax.set_xlabel('Predicted Label')\n",
    "    ax.set_ylabel('True Label')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CONFUSION MATRICES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "plot_confusion_matrix(bert_results[\"labels\"], bert_results[\"predictions\"], \"BERT\")\n",
    "plot_confusion_matrix(deberta_results[\"labels\"], deberta_results[\"predictions\"], \"DeBERTa\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9297926c",
   "metadata": {},
   "source": [
    "## Confusion Matrices\n",
    "\n",
    "Visualize which entity types are frequently confused or misclassified.\n",
    "\n",
    "**Why:** Confusion matrices reveal systematic errors (e.g., ORG frequently predicted as LOC). This guides model improvements.\n",
    "\n",
    "**What:**\n",
    "- Token-level confusion matrix: Entire 9x9 grid showing confusion between all label pairs\n",
    "- Entity-type confusion: Collapse B-/I- distinctions to show ORG/PER/LOC/MISC confusion\n",
    "\n",
    "**How:** \n",
    "1. Flatten predictions and ground truth labels\n",
    "2. Count (true_label, pred_label) pairs\n",
    "3. Visualize as heatmap using seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b01ef69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation on test set\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EVALUATING ON TEST SET\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "def evaluate_on_test(model, tokenizer, model_name, test_sents):\n",
    "    \"\"\"Full evaluation pipeline for test set\"\"\"\n",
    "    print(f\"\\n{model_name.upper()} Test Set Evaluation\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Tokenize test data\n",
    "    test_dataset = TokenizedNERDataset(test_sents, tokenizer, label2id)\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=TRAINING_CONFIG[\"batch_size\"],\n",
    "        shuffle=False,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "    \n",
    "    # Evaluate\n",
    "    start_time = time.time()\n",
    "    test_labels, test_preds, test_loss = evaluate(model, test_loader, device)\n",
    "    inference_time = time.time() - start_time\n",
    "    \n",
    "    # Convert to label strings for seqeval metrics\n",
    "    test_labels_strings = []\n",
    "    test_preds_strings = []\n",
    "    \n",
    "    for label_seq, pred_seq in zip(test_labels, test_preds):\n",
    "        label_strs = [id2label[l] for l in label_seq if l != -100]\n",
    "        pred_strs = [id2label[p for i, p in enumerate(pred_seq) if label_seq[i] != -100][i] \n",
    "                     for i, p in enumerate(pred_seq) if label_seq[i] != -100]\n",
    "        \n",
    "        # Simpler: just filter both sequences\n",
    "        pred_strs = [id2label[p] for i, p in enumerate(pred_seq) if label_seq[i] != -100]\n",
    "        \n",
    "        if label_strs:  # Only add non-empty sequences\n",
    "            test_labels_strings.append(label_strs)\n",
    "            test_preds_strings.append(pred_strs)\n",
    "    \n",
    "    # Compute metrics\n",
    "    precision = precision_score(test_labels_strings, test_preds_strings)\n",
    "    recall = recall_score(test_labels_strings, test_preds_strings)\n",
    "    f1 = f1_score(test_labels_strings, test_preds_strings)\n",
    "    \n",
    "    # Per-entity breakdown\n",
    "    print(\"\\nEntity-Level Metrics:\")\n",
    "    print(classification_report(test_labels_strings, test_preds_strings, digits=4))\n",
    "    print(f\"Overall - Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}\")\n",
    "    print(f\"Inference time: {inference_time:.2f}s for {len(test_preds)} samples\")\n",
    "    \n",
    "    return {\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1,\n",
    "        \"test_loss\": test_loss,\n",
    "        \"inference_time\": inference_time,\n",
    "        \"labels\": test_labels_strings,\n",
    "        \"predictions\": test_preds_strings\n",
    "    }\n",
    "\n",
    "# Evaluate both models\n",
    "bert_results = evaluate_on_test(bert_model, bert_tokenizer, \"BERT\", test_sents)\n",
    "deberta_results = evaluate_on_test(deberta_model, deberta_tokenizer, \"DeBERTa\", test_sents)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60e2ee9",
   "metadata": {},
   "source": [
    "## Evaluation Metrics\n",
    "\n",
    "Compute entity-level and token-level evaluation metrics for both models.\n",
    "\n",
    "**Why:** \n",
    "- **Entity-level**: Strict metric (entire entity must be correct). Reflects real-world NER performance.\n",
    "- **Token-level**: Looser metric (individual tokens). Useful for understanding where models struggle.\n",
    "\n",
    "**What:**\n",
    "- Precision: Of predicted entities, how many are correct?\n",
    "- Recall: Of true entities, how many were found?\n",
    "- F1: Harmonic mean of precision and recall\n",
    "- Per-entity breakdown: Separate scores for LOC, PER, ORG, MISC\n",
    "\n",
    "**How:** Use seqeval for entity-level evaluation (same as Assignment 2), and standard classification metrics for token-level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d4f503",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "TRAINING_CONFIG = {\n",
    "    \"learning_rate\": 2e-5,\n",
    "    \"batch_size\": 16,\n",
    "    \"accumulation_steps\": 2,\n",
    "    \"num_epochs\": 5,\n",
    "    \"warmup_steps\": 500,\n",
    "    \"max_grad_norm\": 1.0,\n",
    "    \"patience\": 3  # early stopping patience\n",
    "}\n",
    "\n",
    "def train_epoch(model, train_loader, optimizer, scheduler, device):\n",
    "    \"\"\"Train for one epoch\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for step, batch in enumerate(tqdm(train_loader, desc=\"Training\")):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels\n",
    "        )\n",
    "        loss = outputs.loss\n",
    "        \n",
    "        # Gradient accumulation\n",
    "        loss = loss / TRAINING_CONFIG[\"accumulation_steps\"]\n",
    "        loss.backward()\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        if (step + 1) % TRAINING_CONFIG[\"accumulation_steps\"] == 0:\n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(\n",
    "                model.parameters(),\n",
    "                TRAINING_CONFIG[\"max_grad_norm\"]\n",
    "            )\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "    \n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "\n",
    "def evaluate(model, eval_loader, device):\n",
    "    \"\"\"Evaluate model on validation/test set\"\"\"\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "    total_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(eval_loader, desc=\"Evaluating\"):\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            \n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels\n",
    "            )\n",
    "            \n",
    "            loss = outputs.loss\n",
    "            logits = outputs.logits\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Get predictions\n",
    "            predictions = logits.argmax(dim=-1).cpu().tolist()\n",
    "            labels_list = labels.cpu().tolist()\n",
    "            \n",
    "            # Convert -100 (ignored) to actual labels for eval\n",
    "            for pred_seq, label_seq in zip(predictions, labels_list):\n",
    "                all_predictions.append(pred_seq)\n",
    "                all_labels.append(label_seq)\n",
    "    \n",
    "    return all_labels, all_predictions, total_loss / len(eval_loader)\n",
    "\n",
    "\n",
    "def fine_tune_model(model, tokenizer, model_name, train_sents, val_sents):\n",
    "    \"\"\"Fine-tune a single model\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"FINE-TUNING {model_name.upper()}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Tokenize data\n",
    "    print(\"Tokenizing data...\")\n",
    "    train_dataset = TokenizedNERDataset(train_sents, tokenizer, label2id)\n",
    "    val_dataset = TokenizedNERDataset(val_sents, tokenizer, label2id)\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=TRAINING_CONFIG[\"batch_size\"],\n",
    "        shuffle=True,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=TRAINING_CONFIG[\"batch_size\"],\n",
    "        shuffle=False,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "    \n",
    "    # Setup optimizer and scheduler\n",
    "    optimizer = AdamW(model.parameters(), lr=TRAINING_CONFIG[\"learning_rate\"])\n",
    "    total_steps = len(train_loader) * TRAINING_CONFIG[\"num_epochs\"] // TRAINING_CONFIG[\"accumulation_steps\"]\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=TRAINING_CONFIG[\"warmup_steps\"],\n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "    \n",
    "    # Training loop with early stopping\n",
    "    best_f1 = -1.0\n",
    "    patience_counter = 0\n",
    "    history = {\"train_loss\": [], \"val_loss\": [], \"val_f1\": []}\n",
    "    \n",
    "    for epoch in range(TRAINING_CONFIG[\"num_epochs\"]):\n",
    "        print(f\"\\nEpoch {epoch+1}/{TRAINING_CONFIG['num_epochs']}\")\n",
    "        \n",
    "        # Train\n",
    "        train_loss = train_epoch(model, train_loader, optimizer, scheduler, device)\n",
    "        history[\"train_loss\"].append(train_loss)\n",
    "        print(f\"  Train loss: {train_loss:.4f}\")\n",
    "        \n",
    "        # Validate\n",
    "        val_labels, val_preds, val_loss = evaluate(model, val_loader, device)\n",
    "        history[\"val_loss\"].append(val_loss)\n",
    "        print(f\"  Val loss: {val_loss:.4f}\")\n",
    "        \n",
    "        # Convert to label strings for seqeval\n",
    "        val_labels_seqeval = [\n",
    "            [id2label[l] for l in seq if l != -100]\n",
    "            for seq in val_labels\n",
    "        ]\n",
    "        val_preds_seqeval = [\n",
    "            [id2label[p] for p in seq if l != -100]\n",
    "            for seq, labels in zip(val_preds, val_labels)\n",
    "            for l in labels\n",
    "        ]\n",
    "        \n",
    "        # Compute F1\n",
    "        try:\n",
    "            val_f1 = f1_score(val_labels_seqeval, val_preds_seqeval)\n",
    "        except:\n",
    "            val_f1 = -1.0  # Edge case\n",
    "        \n",
    "        history[\"val_f1\"].append(val_f1)\n",
    "        print(f\"  Val F1: {val_f1:.4f}\")\n",
    "        \n",
    "        # Early stopping\n",
    "        if val_f1 > best_f1:\n",
    "            best_f1 = val_f1\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), f\"{model_name}_best.pt\")\n",
    "            print(f\"  ✓ Best model saved (F1: {best_f1:.4f})\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= TRAINING_CONFIG[\"patience\"]:\n",
    "                print(f\"  Early stopping (patience {TRAINING_CONFIG['patience']} reached)\")\n",
    "                break\n",
    "    \n",
    "    # Load best model\n",
    "    model.load_state_dict(torch.load(f\"{model_name}_best.pt\"))\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "\n",
    "# Fine-tune both models\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STARTING FINE-TUNING PHASE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "bert_model, bert_history = fine_tune_model(bert_model, bert_tokenizer, \"bert\", train_sents, val_sents)\n",
    "deberta_model, deberta_history = fine_tune_model(deberta_model, deberta_tokenizer, \"deberta\", train_sents, val_sents)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ce20ff",
   "metadata": {},
   "source": [
    "## Fine-tuning Training Loop\n",
    "\n",
    "Set up training infrastructure with gradient clipping, learning rate scheduling, and early stopping.\n",
    "\n",
    "**Why:** Effective fine-tuning of transformers requires:\n",
    "- Appropriate learning rate (LR) to avoid catastrophic forgetting of pretrained weights\n",
    "- Gradient clipping to prevent exploding gradients\n",
    "- Learning rate scheduling (warmup then linear decay) for convergence\n",
    "- Validation-based early stopping to prevent overfitting\n",
    "\n",
    "**What:**\n",
    "- `train_epoch()`: Train for one epoch with gradient accumulation\n",
    "- `evaluate()`: Run inference on validation/test set, compute metrics\n",
    "- Early stopping: Stop if validation F1 doesn't improve for N epochs\n",
    "- Warmup scheduler: Gradual LR increase, then linear decay\n",
    "\n",
    "**How:** For each batch:\n",
    "1. Forward pass through model\n",
    "2. Compute cross-entropy loss (ignoring -100 labels)\n",
    "3. Backward pass and gradient accumulation\n",
    "4. Gradient clipping (max norm = 1.0)\n",
    "5. Update weights\n",
    "6. Track metrics\n",
    "\n",
    "**Notes:**\n",
    "- Different LR for encoder vs classifier (discriminative fine-tuning)\n",
    "- Batch size = 16, gradient accumulation over 2 steps → effective batch size 32\n",
    "- Learning rate = 2e-5 (standard for transformer fine-tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ff8591",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configurations\n",
    "MODEL_CONFIGS = {\n",
    "    \"bert\": {\n",
    "        \"model_name\": \"bert-base-cased\",\n",
    "        \"num_labels\": len(label2id),\n",
    "        \"id2label\": id2label,\n",
    "        \"label2id\": label2id,\n",
    "    },\n",
    "    \"deberta\": {\n",
    "        \"model_name\": \"microsoft/deberta-v3-base\",\n",
    "        \"num_labels\": len(label2id),\n",
    "        \"id2label\": id2label,\n",
    "        \"label2id\": label2id,\n",
    "    }\n",
    "}\n",
    "\n",
    "def load_model_and_tokenizer(model_key):\n",
    "    \"\"\"Load transformer model and tokenizer for token classification\"\"\"\n",
    "    config = MODEL_CONFIGS[model_key]\n",
    "    model_name = config[\"model_name\"]\n",
    "    \n",
    "    print(f\"\\nLoading {model_key.upper()} ({model_name})...\")\n",
    "    \n",
    "    # Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "    # Load model for token classification\n",
    "    model = AutoModelForTokenClassification.from_pretrained(\n",
    "        model_name,\n",
    "        num_labels=config[\"num_labels\"],\n",
    "        id2label=config[\"id2label\"],\n",
    "        label2id=config[\"label2id\"],\n",
    "        ignore_mismatched_sizes=False\n",
    "    )\n",
    "    \n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Count parameters\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    \n",
    "    print(f\"  Total parameters: {total_params:,}\")\n",
    "    print(f\"  Trainable parameters: {trainable_params:,}\")\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "# Load both models\n",
    "print(\"=\" * 60)\n",
    "print(\"LOADING TRANSFORMER MODELS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "bert_model, bert_tokenizer = load_model_and_tokenizer(\"bert\")\n",
    "deberta_model, deberta_tokenizer = load_model_and_tokenizer(\"deberta\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ed4d2c",
   "metadata": {},
   "source": [
    "## Model Selection and Loading\n",
    "\n",
    "Select and load two different transformer-based models for token classification.\n",
    "\n",
    "**Why:** Different transformer architectures have different pretraining objectives, parameter efficiency, and performance characteristics. Comparing them reveals which is better suited for NER.\n",
    "\n",
    "**What:**\n",
    "- **Model 1: BERT (bert-base-cased)** - Classic transformer baseline, bidirectional context, 12 layers, 110M parameters\n",
    "- **Model 2: DeBERTa (deberta-v3-base)** - Modern transformer variant with disentangled attention, 12 layers, 86M parameters (more efficient)\n",
    "\n",
    "Both loaded with AutoModelForTokenClassification for the NER task (9 labels for CoNLL-2003).\n",
    "\n",
    "**How:** Load pretrained weights and modify the classification head to output 9 labels (one per tag type)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538fe639",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization and dataset class\n",
    "class TokenizedNERDataset(Dataset):\n",
    "    def __init__(self, sentences, tokenizer, label2id, max_length=512):\n",
    "        \"\"\"\n",
    "        sentences: list of (tokens, pos_tags, labels)\n",
    "        tokenizer: from transformers (e.g., AutoTokenizer)\n",
    "        label2id: dict mapping label strings to IDs\n",
    "        \"\"\"\n",
    "        self.tokenizer = tokenizer\n",
    "        self.label2id = label2id\n",
    "        self.max_length = max_length\n",
    "        self.encoded_data = []\n",
    "        \n",
    "        for tokens, _, labels in sentences:\n",
    "            # Tokenize with word_ids tracking\n",
    "            encoding = tokenizer(\n",
    "                tokens,\n",
    "                truncation=True,\n",
    "                max_length=max_length,\n",
    "                padding='max_length',\n",
    "                is_split_into_words=True,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "            \n",
    "            # Align labels to subword tokens\n",
    "            word_ids = encoding.word_ids()\n",
    "            label_ids = [-100] * len(word_ids)  # -100 = ignore in loss\n",
    "            \n",
    "            for idx, word_id in enumerate(word_ids):\n",
    "                if word_id is not None:\n",
    "                    # Only assign label to first subword of each word\n",
    "                    if idx == 0 or encoding.word_ids()[idx-1] != word_id:\n",
    "                        label_ids[idx] = label2id.get(labels[word_id], label2id.get(\"O\", 0))\n",
    "            \n",
    "            self.encoded_data.append({\n",
    "                \"input_ids\": encoding[\"input_ids\"].squeeze(),\n",
    "                \"attention_mask\": encoding[\"attention_mask\"].squeeze(),\n",
    "                \"labels\": torch.tensor(label_ids, dtype=torch.long)\n",
    "            })\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.encoded_data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.encoded_data[idx]\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"Collate batch for DataLoader\"\"\"\n",
    "    input_ids = torch.stack([item[\"input_ids\"] for item in batch])\n",
    "    attention_mask = torch.stack([item[\"attention_mask\"] for item in batch])\n",
    "    labels = torch.stack([item[\"labels\"] for item in batch])\n",
    "    \n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels\": labels\n",
    "    }\n",
    "\n",
    "\n",
    "# Create datasets for both models\n",
    "print(\"Tokenizing data (this may take a moment)...\")\n",
    "# Will be done per-model since each uses different tokenizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3836226f",
   "metadata": {},
   "source": [
    "## Tokenization and Dataset Class\n",
    "\n",
    "Create a PyTorch dataset that handles subword token alignment for transformer models.\n",
    "\n",
    "**Why:** Transformers tokenize text differently than word tokenizers. \"New York\" might become [\"New\", \"Y\", \"##ork\"], requiring label alignment.\n",
    "\n",
    "**What:**\n",
    "- `TokenizedNERDataset`: PyTorch dataset that tokenizes sentences and aligns labels with subword tokens\n",
    "- `word_ids()`: Method that maps subword tokens back to original words for evaluation\n",
    "- Batch collation: Pads sequences to max length in batch\n",
    "\n",
    "**How:** For each subword token created by the transformer tokenizer, check if it's part of a word. First subword of a word gets the word's label; continuation subwords get ignored (-100) during loss computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c69fcfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CoNLL-2003 from Assignment 2\n",
    "cwd = Path.cwd()\n",
    "local_dir = Path(\"Assignment2_Name_Entity_Recognition/conll2003\")\n",
    "alt_dir = Path(\"conll2003\")\n",
    "parent_dir = Path(\"../Assignment2_Name_Entity_Recognition/conll2003\")\n",
    "\n",
    "if (cwd / local_dir).exists():\n",
    "    data_dir = cwd / local_dir\n",
    "elif (cwd / alt_dir).exists():\n",
    "    data_dir = cwd / alt_dir\n",
    "elif (cwd / parent_dir).exists():\n",
    "    data_dir = cwd / parent_dir\n",
    "else:\n",
    "    raise FileNotFoundError(\"Cannot find conll2003 folder\")\n",
    "\n",
    "train_path = data_dir / \"eng.train\"\n",
    "val_path = data_dir / \"eng.testa\"\n",
    "test_path = data_dir / \"eng.testb\"\n",
    "\n",
    "# Parse CoNLL format (same as Assignment 2)\n",
    "def read_conll(path):\n",
    "    sents = []\n",
    "    tokens = []\n",
    "    pos_tags = []\n",
    "    labels = []\n",
    "    with path.open(\"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                if tokens:\n",
    "                    sents.append((tokens, pos_tags, labels))\n",
    "                    tokens, pos_tags, labels = [], [], []\n",
    "                continue\n",
    "            if line.startswith(\"-DOCSTART-\"):\n",
    "                continue\n",
    "            parts = line.split()\n",
    "            if len(parts) < 4:\n",
    "                continue\n",
    "            token, pos, _chunk, ner = parts[0], parts[1], parts[2], parts[3]\n",
    "            tokens.append(token)\n",
    "            pos_tags.append(pos)\n",
    "            labels.append(ner)\n",
    "    if tokens:\n",
    "        sents.append((tokens, pos_tags, labels))\n",
    "    return sents\n",
    "\n",
    "train_sents = read_conll(train_path)\n",
    "val_sents = read_conll(val_path)\n",
    "test_sents = read_conll(test_path)\n",
    "\n",
    "# Extract all labels\n",
    "label_set = set()\n",
    "for _, _, labels in train_sents:\n",
    "    label_set.update(labels)\n",
    "label2id = {label: idx for idx, label in enumerate(sorted(label_set))}\n",
    "id2label = {idx: label for label, idx in label2id.items()}\n",
    "\n",
    "print(f\"Dataset: CoNLL-2003\")\n",
    "print(f\"Labels: {list(label2id.keys())}\")\n",
    "print(f\"Train: {len(train_sents)}, Val: {len(val_sents)}, Test: {len(test_sents)}\")\n",
    "print(f\"Label mapping: {label2id}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d6cef2",
   "metadata": {},
   "source": [
    "## Data Loading and Preparation\n",
    "\n",
    "Load the CoNLL-2003 dataset from Assignment 2 and prepare it for transformer models.\n",
    "\n",
    "**Why:** Transformers use subword tokenization (different from Assignment 2's word-level tokens). We must align BIO labels with subword tokens.\n",
    "\n",
    "**What:**\n",
    "- `read_conll()`: Parse CoNLL-2003 files (same as Assignment 2)\n",
    "- `TokenizedNERDataset`: PyTorch dataset that handles subword token alignment\n",
    "- Split data into train/validation/test\n",
    "- Track token-to-label alignment for evaluation\n",
    "\n",
    "**How:** For each subword token, inherit the label from its parent word. For example, \"New York\" → \"New\" (B-LOC) + \"York\" (I-LOC) becomes \"New\" (B-LOC) + \"▁York\" (I-LOC) after subword tokenization, preserving the I-LOC label.\n",
    "\n",
    "**Note:** This is more complex than Assignment 2 because transformers use dynamic vocabularies; we must carefully align predictions back to original words for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18d24ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim import AdamW\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForTokenClassification,\n",
    "    get_linear_schedule_with_warmup, Trainer, TrainingArguments\n",
    ")\n",
    "from seqeval.metrics import classification_report, f1_score, precision_score, recall_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Device setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09726935",
   "metadata": {},
   "source": [
    "## Setup: Imports and Configuration\n",
    "\n",
    "Load required libraries and set random seeds for reproducibility.\n",
    "\n",
    "**Why:** Transformers use random initialization. Fix seeds to ensure reproducible results across runs.\n",
    "\n",
    "**What:** Import:\n",
    "- `transformers`: For model loading, tokenizers, and training utilities\n",
    "- `torch`: For GPU/CPU detection and model operations\n",
    "- `seqeval.metrics`: For entity-level NER evaluation\n",
    "- `sklearn`: For confusion matrices and classification metrics\n",
    "- `tqdm`: For progress tracking\n",
    "\n",
    "**How:** Set random seeds (42) for Python, NumPy, PyTorch, and Hugging Face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8234a6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (run once)\n",
    "!pip -q install transformers torch datasets seqeval scikit-learn matplotlib seaborn pandas numpy tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6fe787c",
   "metadata": {},
   "source": [
    "## Setup: Dependencies\n",
    "\n",
    "Install required packages for transformer models, NER evaluation, and visualization.\n",
    "\n",
    "**Why:** This assignment uses Hugging Face transformers for model access, PyTorch for training, and additional tools for evaluation and visualization.\n",
    "\n",
    "**What:** Install packages for:\n",
    "- `transformers` and `datasets`: Hugging Face ecosystem for pretrained models and data handling\n",
    "- `torch`: Neural network framework\n",
    "- `seqeval`: Entity-level NER metrics (same as Assignment 2)\n",
    "- `scikit-learn`: For confusion matrices and additional metrics\n",
    "- `matplotlib` and `seqeval`: For visualization\n",
    "\n",
    "**Note:** Run this once at the beginning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027f295a",
   "metadata": {},
   "source": [
    "# Assignment 3: Transformer-Based NER Models\n",
    "\n",
    "This notebook fine-tunes and compares two transformer-based models (BERT and DeBERTa) for Named Entity Recognition on the CoNLL-2003 dataset.\n",
    "\n",
    "**Goals:**\n",
    "- Fine-tune two different transformer architectures for token classification\n",
    "- Evaluate performance using entity-level and token-level metrics\n",
    "- Analyze misclassified entities and identify systematic errors\n",
    "- Compare models on accuracy, speed, and computational efficiency\n",
    "- Understand how pretraining impacts NER performance"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
